{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug & Fix Qwen3-Reranker-8B\n",
    "\n",
    "Re-rankers SHOULD outperform embeddings for:\n",
    "- Ambiguous queries\n",
    "- Context-dependent questions\n",
    "- Precise keyword matching\n",
    "\n",
    "Let's find and fix why yours isn't working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ“ Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RERANKER ENDPOINT CONFIG\n",
    "# =============================================================================\n",
    "RERANKER_ENDPOINT = 'your-qwen3-reranker-endpoint'  # <-- UPDATE THIS\n",
    "\n",
    "reranker_client = boto3.client(\n",
    "    'sagemaker-runtime',\n",
    "    region_name=config['models']['embedding']['credentials']['region'],\n",
    "    aws_access_key_id=config['models']['embedding']['credentials']['accessKeyId'],\n",
    "    aws_secret_access_key=config['models']['embedding']['credentials']['secretAccessKey'],\n",
    "    aws_session_token=config['models']['embedding']['credentials']['sessionToken']\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Client ready for endpoint: {RERANKER_ENDPOINT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Test Different Input Formats\n",
    "\n",
    "Different inference engines expect different formats. Let's find what works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data - Doc 0 is CLEARLY most relevant\n",
    "test_query = \"How do I cancel a flight booking?\"\n",
    "\n",
    "test_documents = [\n",
    "    \"To cancel a flight booking, log into your account, go to My Trips, select the booking, and click Cancel. You will receive a confirmation email within 24 hours.\",  # MOST RELEVANT\n",
    "    \"Our refund policy allows full refunds within 24 hours of booking. After that, cancellation fees may apply depending on your fare type.\",  # SOMEWHAT RELEVANT\n",
    "    \"To book a new flight, search for your destination, select dates, choose seats, and complete payment. You can pay with credit card or PayPal.\"  # NOT RELEVANT\n",
    "]\n",
    "\n",
    "print(\"Test data:\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"\\nDoc 0 (MOST relevant): {test_documents[0][:80]}...\")\n",
    "print(f\"Doc 1 (somewhat relevant): {test_documents[1][:80]}...\")\n",
    "print(f\"Doc 2 (NOT relevant): {test_documents[2][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reranker_format(payload: dict, format_name: str):\n",
    "    \"\"\"Test a specific input format\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing format: {format_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Payload keys: {payload.keys()}\")\n",
    "    \n",
    "    try:\n",
    "        response = reranker_client.invoke_endpoint(\n",
    "            EndpointName=RERANKER_ENDPOINT,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        raw_bytes = response['Body'].read()\n",
    "        print(f\"Response length: {len(raw_bytes)} bytes\")\n",
    "        \n",
    "        if len(raw_bytes) == 0:\n",
    "            print(\"âŒ Empty response\")\n",
    "            return None\n",
    "        \n",
    "        output = json.loads(raw_bytes.decode('utf-8'))\n",
    "        print(f\"âœ“ Got response\")\n",
    "        print(f\"Response structure: {type(output)}\")\n",
    "        \n",
    "        if isinstance(output, dict):\n",
    "            print(f\"Keys: {output.keys()}\")\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 1: query + documents\n",
    "payload1 = {\n",
    "    \"query\": test_query,\n",
    "    \"documents\": test_documents\n",
    "}\n",
    "result1 = test_reranker_format(payload1, \"query + documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 2: pairs\n",
    "payload2 = {\n",
    "    \"pairs\": [[test_query, doc] for doc in test_documents]\n",
    "}\n",
    "result2 = test_reranker_format(payload2, \"pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 3: inputs (HuggingFace style)\n",
    "payload3 = {\n",
    "    \"inputs\": [[test_query, doc] for doc in test_documents]\n",
    "}\n",
    "result3 = test_reranker_format(payload3, \"inputs (HF style)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 4: query + passages\n",
    "payload4 = {\n",
    "    \"query\": test_query,\n",
    "    \"passages\": test_documents\n",
    "}\n",
    "result4 = test_reranker_format(payload4, \"query + passages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 5: query + texts\n",
    "payload5 = {\n",
    "    \"query\": test_query,\n",
    "    \"texts\": test_documents\n",
    "}\n",
    "result5 = test_reranker_format(payload5, \"query + texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORMAT 6: Qwen reranker specific format with document objects\n",
    "payload6 = {\n",
    "    \"query\": test_query,\n",
    "    \"documents\": [{\"text\": doc} for doc in test_documents]\n",
    "}\n",
    "result6 = test_reranker_format(payload6, \"query + document objects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analyze the Response\n",
    "\n",
    "Use the format that worked above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the format that worked (update this based on above results)\n",
    "# For your response format, it looks like FORMAT 1 or 6 worked\n",
    "\n",
    "working_result = result1  # <-- Change to the one that worked (result1, result2, etc.)\n",
    "\n",
    "print(\"Full response:\")\n",
    "print(json.dumps(working_result, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and analyze scores\n",
    "if working_result and 'results' in working_result:\n",
    "    results = working_result['results']\n",
    "    \n",
    "    print(\"\\nScore Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    scores = []\n",
    "    for r in results:\n",
    "        idx = r['index']\n",
    "        score = r['relevance_score']\n",
    "        scores.append((idx, score))\n",
    "        \n",
    "        relevance = \"MOST RELEVANT\" if idx == 0 else (\"SOMEWHAT\" if idx == 1 else \"NOT RELEVANT\")\n",
    "        print(f\"  Doc {idx}: score = {score:.6f} ({relevance})\")\n",
    "    \n",
    "    # Check score order\n",
    "    scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    print(f\"\\nRanking by score (highest first): {[s[0] for s in scores_sorted]}\")\n",
    "    \n",
    "    if scores_sorted[0][0] == 0:\n",
    "        print(\"\\nâœ“ CORRECT! Doc 0 (most relevant) has highest score\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ WRONG! Doc {scores_sorted[0][0]} ranked first, should be Doc 0\")\n",
    "        \n",
    "        # Check if lower is better\n",
    "        scores_sorted_asc = sorted(scores, key=lambda x: x[1])\n",
    "        if scores_sorted_asc[0][0] == 0:\n",
    "            print(\"\\nðŸ’¡ TRY: Lower scores = better! Use score_order='asc'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Test with Conversation History\n",
    "\n",
    "This is where re-ranker should shine - understanding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: Ambiguous query WITH context\n",
    "# The re-ranker should understand \"documents\" means cancellation documents\n",
    "\n",
    "# Build a context-enriched query\n",
    "history_context = \"Previous conversation: User asked about cancelling a booking. User asked about refund policy.\"\n",
    "ambiguous_query = \"What documents do I need?\"\n",
    "\n",
    "# Option A: Prepend context to query\n",
    "enriched_query_a = f\"{history_context} Current question: {ambiguous_query}\"\n",
    "\n",
    "# Option B: Simpler - just key terms\n",
    "enriched_query_b = f\"cancellation refund booking - {ambiguous_query}\"\n",
    "\n",
    "# Option C: Question with context hint\n",
    "enriched_query_c = f\"Regarding booking cancellation and refunds: {ambiguous_query}\"\n",
    "\n",
    "print(\"Context-enriched queries:\")\n",
    "print(f\"\\nOption A (verbose): {enriched_query_a}\")\n",
    "print(f\"\\nOption B (keywords): {enriched_query_b}\")\n",
    "print(f\"\\nOption C (natural): {enriched_query_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test documents for ambiguous query\n",
    "context_test_docs = [\n",
    "    \"To process a cancellation, you need: booking confirmation number, passenger ID, and the original payment receipt. Submit these via the cancellation form.\",  # RELEVANT with context\n",
    "    \"Required documents for new bookings include: valid passport, credit card, and contact information.\",  # NOT relevant with context\n",
    "    \"General documentation policy: All documents must be in PDF format and under 5MB.\",  # NOT relevant\n",
    "]\n",
    "\n",
    "def test_with_query(query: str, docs: list, label: str):\n",
    "    \"\"\"Test re-ranker with a specific query\"\"\"\n",
    "    payload = {\n",
    "        \"query\": query,\n",
    "        \"documents\": docs\n",
    "    }\n",
    "    \n",
    "    response = reranker_client.invoke_endpoint(\n",
    "        EndpointName=RERANKER_ENDPOINT,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "    results = output['results']\n",
    "    \n",
    "    print(f\"\\n{label}\")\n",
    "    print(f\"Query: {query[:80]}...\" if len(query) > 80 else f\"Query: {query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    scores = [(r['index'], r['relevance_score']) for r in results]\n",
    "    scores_sorted = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for idx, score in scores_sorted:\n",
    "        print(f\"  Rank: Doc {idx} (score: {score:.4f}) - {docs[idx][:50]}...\")\n",
    "    \n",
    "    return scores_sorted[0][0]  # Return top-ranked doc index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test without context (should maybe fail)\n",
    "top_no_context = test_with_query(\n",
    "    ambiguous_query, \n",
    "    context_test_docs, \n",
    "    \"WITHOUT CONTEXT\"\n",
    ")\n",
    "\n",
    "# Test with context Option A\n",
    "top_context_a = test_with_query(\n",
    "    enriched_query_a, \n",
    "    context_test_docs, \n",
    "    \"WITH CONTEXT (Option A - verbose)\"\n",
    ")\n",
    "\n",
    "# Test with context Option B  \n",
    "top_context_b = test_with_query(\n",
    "    enriched_query_b, \n",
    "    context_test_docs, \n",
    "    \"WITH CONTEXT (Option B - keywords)\"\n",
    ")\n",
    "\n",
    "# Test with context Option C\n",
    "top_context_c = test_with_query(\n",
    "    enriched_query_c, \n",
    "    context_test_docs, \n",
    "    \"WITH CONTEXT (Option C - natural)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Expected top doc: 0 (cancellation documents)\")\n",
    "print(f\"Without context: Doc {top_no_context} ranked first\")\n",
    "print(f\"With context A:  Doc {top_context_a} ranked first\")\n",
    "print(f\"With context B:  Doc {top_context_b} ranked first\")\n",
    "print(f\"With context C:  Doc {top_context_c} ranked first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Find the Best Context Format\n",
    "\n",
    "Based on the above, identify which context format works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best format\n",
    "results_summary = {\n",
    "    'no_context': top_no_context,\n",
    "    'verbose': top_context_a,\n",
    "    'keywords': top_context_b,\n",
    "    'natural': top_context_c\n",
    "}\n",
    "\n",
    "correct = [k for k, v in results_summary.items() if v == 0]\n",
    "\n",
    "if correct:\n",
    "    print(f\"âœ“ These context formats work: {correct}\")\n",
    "    print(f\"\\nRecommended: Use '{correct[0]}' format for building context-enriched queries\")\n",
    "else:\n",
    "    print(\"âŒ None of the formats produced correct ranking\")\n",
    "    print(\"\\nPossible issues:\")\n",
    "    print(\"  1. Re-ranker model may not be suitable for your domain\")\n",
    "    print(\"  2. Document content may need preprocessing\")\n",
    "    print(\"  3. Consider using embedding-based approach instead\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Working Implementation\n",
    "\n",
    "Based on what we learned, here's the optimized re-ranker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedQwen3Reranker:\n",
    "    \"\"\"\n",
    "    Optimized Qwen3-Reranker with proper context handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint_name: str, client):\n",
    "        self.endpoint_name = endpoint_name\n",
    "        self.client = client\n",
    "        \n",
    "        # Set based on your testing above\n",
    "        self.context_format = 'natural'  # 'verbose', 'keywords', or 'natural'\n",
    "    \n",
    "    def build_contextualized_query(self, query: str, history: list) -> str:\n",
    "        \"\"\"\n",
    "        Build query with conversation history context.\n",
    "        \"\"\"\n",
    "        if not history:\n",
    "            return query\n",
    "        \n",
    "        recent = history[-3:]  # Last 3 turns\n",
    "        \n",
    "        if self.context_format == 'verbose':\n",
    "            # Detailed context\n",
    "            context_parts = [f\"User asked: {h.get('query', '')}\" for h in recent]\n",
    "            context = \" \".join(context_parts)\n",
    "            return f\"Previous conversation: {context}. Current question: {query}\"\n",
    "        \n",
    "        elif self.context_format == 'keywords':\n",
    "            # Just key terms\n",
    "            keywords = \" \".join([h.get('query', '') for h in recent])\n",
    "            return f\"{keywords} - {query}\"\n",
    "        \n",
    "        else:  # 'natural'\n",
    "            # Natural phrasing\n",
    "            topics = \" and \".join([h.get('query', '').lower() for h in recent[-2:]])\n",
    "            return f\"Regarding {topics}: {query}\"\n",
    "    \n",
    "    def rerank(self, query: str, documents: list, history: list = None) -> list:\n",
    "        \"\"\"\n",
    "        Re-rank documents with optional conversation history.\n",
    "        \n",
    "        Returns: List of (index, score) tuples sorted by relevance\n",
    "        \"\"\"\n",
    "        # Build contextualized query if history provided\n",
    "        final_query = self.build_contextualized_query(query, history) if history else query\n",
    "        \n",
    "        # Call API\n",
    "        payload = {\n",
    "            \"query\": final_query,\n",
    "            \"documents\": documents\n",
    "        }\n",
    "        \n",
    "        response = self.client.invoke_endpoint(\n",
    "            EndpointName=self.endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        output = json.loads(response['Body'].read().decode('utf-8'))\n",
    "        results = output['results']\n",
    "        \n",
    "        # Extract scores sorted by index\n",
    "        scores = [(r['index'], r['relevance_score']) for r in results]\n",
    "        scores.sort(key=lambda x: x[0])  # Sort by index\n",
    "        \n",
    "        return [score for idx, score in scores]\n",
    "\n",
    "print(\"âœ“ OptimizedQwen3Reranker class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the optimized reranker\n",
    "reranker = OptimizedQwen3Reranker(RERANKER_ENDPOINT, reranker_client)\n",
    "\n",
    "# Set the context format that worked best (from Step 4)\n",
    "reranker.context_format = 'natural'  # Update based on your results\n",
    "\n",
    "# Test with history\n",
    "test_history = [\n",
    "    {'query': 'How do I cancel a booking?'},\n",
    "    {'query': 'What is the refund policy?'}\n",
    "]\n",
    "\n",
    "scores = reranker.rerank(\n",
    "    query=\"What documents do I need?\",\n",
    "    documents=context_test_docs,\n",
    "    history=test_history\n",
    ")\n",
    "\n",
    "print(\"Final test with optimized reranker:\")\n",
    "print(f\"Scores: {scores}\")\n",
    "\n",
    "# Rank documents\n",
    "ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "print(f\"Ranking: {ranked_indices}\")\n",
    "print(f\"\\nTop document: Doc {ranked_indices[0]}\")\n",
    "print(f\"Expected: Doc 0 (cancellation documents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "If the re-ranker still doesn't work correctly after these tests:\n",
    "\n",
    "1. **The model may not be well-suited** for your specific document domain\n",
    "2. **Consider fine-tuning** the re-ranker on your data\n",
    "3. **Use the embedding-based approach** with query expansion (which you already have)\n",
    "\n",
    "But if one of the context formats worked, update your retriever to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DEBUG COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Check which context format worked best above\")\n",
    "print(\"2. Update reranker.context_format in your code\")\n",
    "print(\"3. If none worked, use embedding-based query expansion\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

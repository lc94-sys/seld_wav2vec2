{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Industry-Standard Approaches: Hybrid Scoring vs RRF\n",
    "\n",
    "This notebook implements both approaches used in production:\n",
    "\n",
    "## 1. Weighted Score Fusion\n",
    "```\n",
    "final_score = α × embedding_score + β × reranker_score\n",
    "```\n",
    "Used by: Pinecone, many RAG systems\n",
    "\n",
    "## 2. Reciprocal Rank Fusion (RRF)\n",
    "```\n",
    "RRF_score = 1/(k + rank_embedding) + 1/(k + rank_reranker)\n",
    "```\n",
    "Used by: Elasticsearch, Azure AI Search, Weaviate\n",
    "\n",
    "## 3. Re-ranker Only (for comparison)\n",
    "```\n",
    "final_score = reranker_score\n",
    "```\n",
    "Used by: Cohere Rerank, when re-ranker is highly tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Literal\n",
    "import numpy as np\n",
    "import faiss\n",
    "import boto3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RERANKER_ENDPOINT = 'your-qwen3-reranker-endpoint'  # <-- UPDATE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionRetriever:\n",
    "    \"\"\"\n",
    "    Production-grade retriever with multiple fusion strategies:\n",
    "    \n",
    "    1. 'weighted' - Weighted score fusion (α × embed + β × rerank)\n",
    "    2. 'rrf' - Reciprocal Rank Fusion (industry standard)\n",
    "    3. 'reranker_only' - Trust re-ranker completely\n",
    "    4. 'embedding_only' - No re-ranking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, reranker_endpoint: str):\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding client\n",
    "        self.embedding_endpoint_name = config['models']['embedding']['endpoint_name']\n",
    "        embedding_creds = config['models']['embedding']['credentials']\n",
    "        self.embedding_client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=embedding_creds['region'],\n",
    "            aws_access_key_id=embedding_creds['accessKeyId'],\n",
    "            aws_secret_access_key=embedding_creds['secretAccessKey'],\n",
    "            aws_session_token=embedding_creds['sessionToken']\n",
    "        )\n",
    "        print(\"✓ Embedding client initialized\")\n",
    "        \n",
    "        # Re-ranker client\n",
    "        self.reranker_endpoint = reranker_endpoint\n",
    "        self.reranker_client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=embedding_creds['region'],\n",
    "            aws_access_key_id=embedding_creds['accessKeyId'],\n",
    "            aws_secret_access_key=embedding_creds['secretAccessKey'],\n",
    "            aws_session_token=embedding_creds['sessionToken']\n",
    "        )\n",
    "        print(\"✓ Re-ranker client initialized\")\n",
    "        \n",
    "        self.sessions = {}\n",
    "        self.load_indexes()\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        faiss_path = os.path.join(self.config['storage']['faiss_index'], 'faiss.index')\n",
    "        self.faiss_index = faiss.read_index(faiss_path)\n",
    "        \n",
    "        bm25_path = os.path.join(self.config['storage']['bm25_index'], 'bm25.pkl')\n",
    "        with open(bm25_path, 'rb') as f:\n",
    "            self.bm25_index = pickle.load(f)\n",
    "        \n",
    "        metadata_path = os.path.join(self.config['storage']['faiss_index'], 'chunk_metadata.json')\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        \n",
    "        print(f\"✓ Indexes loaded: {len(self.chunks)} chunks\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        params = {\"inputs\": [text], \"encoding_format\": \"float\"}\n",
    "        response = self.embedding_client.invoke_endpoint(\n",
    "            EndpointName=self.embedding_endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(params)\n",
    "        )\n",
    "        raw_bytes = response['Body'].read()\n",
    "        output_data = json.loads(raw_bytes.decode())\n",
    "        return np.array(output_data[0], dtype='float32')\n",
    "    \n",
    "    def get_rerank_scores(self, query: str, documents: List[str]) -> List[float]:\n",
    "        payload = {\"query\": query, \"documents\": documents}\n",
    "        response = self.reranker_client.invoke_endpoint(\n",
    "            EndpointName=self.reranker_endpoint,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        raw_bytes = response['Body'].read()\n",
    "        output_data = json.loads(raw_bytes.decode('utf-8'))\n",
    "        \n",
    "        results = output_data['results']\n",
    "        scores_with_index = [(r['index'], r['relevance_score']) for r in results]\n",
    "        scores_with_index.sort(key=lambda x: x[0])\n",
    "        return [score for idx, score in scores_with_index]\n",
    "    \n",
    "    def build_context_query(self, query: str, history: List[Dict]) -> str:\n",
    "        if not history:\n",
    "            return query\n",
    "        recent = history[-3:]\n",
    "        context_queries = [h.get('query', '') for h in recent if h.get('query')]\n",
    "        if not context_queries:\n",
    "            return query\n",
    "        context = \" and \".join(context_queries[-2:]).lower()\n",
    "        return f\"Regarding {context}: {query}\"\n",
    "    \n",
    "    def hybrid_search(self, query: str, entitlement: str, org_id: str = None,\n",
    "                      tags: List[str] = None, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"Embedding-based hybrid search (vector + BM25)\"\"\"\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        initial_top_k = min(top_k * 5, len(self.chunks))\n",
    "        \n",
    "        vector_scores, vector_indices = self.faiss_index.search(query_embedding, initial_top_k)\n",
    "        vector_scores = vector_scores[0]\n",
    "        vector_indices = vector_indices[0]\n",
    "        \n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        vector_scores_norm = normalize(vector_scores)\n",
    "        bm25_scores_norm = normalize(bm25_scores)\n",
    "        \n",
    "        vector_weight = self.config['retrieval']['hybrid']['vector_weight']\n",
    "        bm25_weight = self.config['retrieval']['hybrid']['bm25_weight']\n",
    "        \n",
    "        hybrid_scores = {}\n",
    "        for idx, score in zip(vector_indices, vector_scores_norm):\n",
    "            hybrid_scores[idx] = score * vector_weight\n",
    "        \n",
    "        for idx, score in enumerate(bm25_scores_norm):\n",
    "            if idx in hybrid_scores:\n",
    "                hybrid_scores[idx] += score * bm25_weight\n",
    "            else:\n",
    "                hybrid_scores[idx] = score * bm25_weight\n",
    "        \n",
    "        sorted_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for idx, score in sorted_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            \n",
    "            chunk_entitlements = chunk['entitlement']\n",
    "            if isinstance(chunk_entitlements, str):\n",
    "                chunk_entitlements = [chunk_entitlements]\n",
    "            \n",
    "            has_access = 'universal' in chunk_entitlements or entitlement in chunk_entitlements\n",
    "            if not has_access:\n",
    "                continue\n",
    "            if org_id and chunk['orgId'] != org_id:\n",
    "                continue\n",
    "            if tags and not any(t in chunk['metadata']['tags'] for t in tags):\n",
    "                continue\n",
    "            \n",
    "            chunk['hybrid_score'] = float(score)\n",
    "            chunk['chunk_idx'] = idx\n",
    "            results.append(chunk)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FUSION STRATEGIES\n",
    "    # =========================================================================\n",
    "    \n",
    "    def fuse_weighted(self, candidates: List[Dict], rerank_scores: List[float],\n",
    "                      embedding_weight: float = 0.6, reranker_weight: float = 0.4) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        STRATEGY 1: Weighted Score Fusion\n",
    "        \n",
    "        final_score = α × normalized_embedding + β × normalized_rerank\n",
    "        \n",
    "        Used by: Pinecone, many production RAG systems\n",
    "        \"\"\"\n",
    "        hybrid_scores = np.array([c['hybrid_score'] for c in candidates])\n",
    "        rerank_scores = np.array(rerank_scores)\n",
    "        \n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.ones_like(scores) * 0.5\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        hybrid_norm = normalize(hybrid_scores)\n",
    "        rerank_norm = normalize(rerank_scores)\n",
    "        \n",
    "        for i, chunk in enumerate(candidates):\n",
    "            chunk['rerank_score'] = float(rerank_scores[i])\n",
    "            chunk['final_score'] = embedding_weight * hybrid_norm[i] + reranker_weight * rerank_norm[i]\n",
    "            chunk['fusion_method'] = 'weighted'\n",
    "        \n",
    "        candidates.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        return candidates\n",
    "    \n",
    "    def fuse_rrf(self, candidates: List[Dict], rerank_scores: List[float],\n",
    "                 k: int = 60) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        STRATEGY 2: Reciprocal Rank Fusion (RRF)\n",
    "        \n",
    "        RRF_score = 1/(k + rank_embedding) + 1/(k + rank_reranker)\n",
    "        \n",
    "        Used by: Elasticsearch, Azure AI Search, Weaviate\n",
    "        Industry standard for combining multiple rankings.\n",
    "        \n",
    "        k=60 is the standard value (from original paper)\n",
    "        \"\"\"\n",
    "        # Get embedding ranking (already sorted by hybrid_score)\n",
    "        embedding_ranks = {i: rank for rank, i in enumerate(range(len(candidates)))}\n",
    "        \n",
    "        # Get reranker ranking\n",
    "        rerank_with_idx = [(i, score) for i, score in enumerate(rerank_scores)]\n",
    "        rerank_with_idx.sort(key=lambda x: x[1], reverse=True)\n",
    "        reranker_ranks = {idx: rank for rank, (idx, _) in enumerate(rerank_with_idx)}\n",
    "        \n",
    "        # Compute RRF score\n",
    "        for i, chunk in enumerate(candidates):\n",
    "            emb_rank = embedding_ranks[i]\n",
    "            rerank_rank = reranker_ranks[i]\n",
    "            \n",
    "            rrf_score = (1 / (k + emb_rank)) + (1 / (k + rerank_rank))\n",
    "            \n",
    "            chunk['rerank_score'] = float(rerank_scores[i])\n",
    "            chunk['embedding_rank'] = emb_rank\n",
    "            chunk['reranker_rank'] = rerank_rank\n",
    "            chunk['final_score'] = rrf_score\n",
    "            chunk['fusion_method'] = 'rrf'\n",
    "        \n",
    "        candidates.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        return candidates\n",
    "    \n",
    "    def fuse_reranker_only(self, candidates: List[Dict], rerank_scores: List[float]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        STRATEGY 3: Re-ranker Only\n",
    "        \n",
    "        Trust re-ranker completely for final ranking.\n",
    "        \n",
    "        Used by: Cohere Rerank, when re-ranker is highly tuned\n",
    "        \"\"\"\n",
    "        for i, chunk in enumerate(candidates):\n",
    "            chunk['rerank_score'] = float(rerank_scores[i])\n",
    "            chunk['final_score'] = float(rerank_scores[i])\n",
    "            chunk['fusion_method'] = 'reranker_only'\n",
    "        \n",
    "        candidates.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        return candidates\n",
    "    \n",
    "    def fuse_embedding_only(self, candidates: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        STRATEGY 4: Embedding Only (no re-ranking)\n",
    "        \"\"\"\n",
    "        for chunk in candidates:\n",
    "            chunk['final_score'] = chunk['hybrid_score']\n",
    "            chunk['fusion_method'] = 'embedding_only'\n",
    "        \n",
    "        # Already sorted by hybrid_score\n",
    "        return candidates\n",
    "    \n",
    "    # =========================================================================\n",
    "    # MAIN QUERY METHOD\n",
    "    # =========================================================================\n",
    "    \n",
    "    def query(self, query: str, entitlement: str, org_id: str = None,\n",
    "              tags: List[str] = None, top_k: int = 5,\n",
    "              candidates_for_rerank: int = 20,\n",
    "              conversation_history: List[Dict] = None,\n",
    "              fusion_method: Literal['weighted', 'rrf', 'reranker_only', 'embedding_only'] = 'rrf',\n",
    "              embedding_weight: float = 0.6,\n",
    "              reranker_weight: float = 0.4,\n",
    "              rrf_k: int = 60,\n",
    "              verbose: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Query with configurable fusion strategy.\n",
    "        \n",
    "        Args:\n",
    "            fusion_method: 'weighted', 'rrf', 'reranker_only', or 'embedding_only'\n",
    "            embedding_weight: Weight for embeddings (only for 'weighted')\n",
    "            reranker_weight: Weight for re-ranker (only for 'weighted')\n",
    "            rrf_k: K parameter for RRF (default 60)\n",
    "        \"\"\"\n",
    "        # Stage 1: Embedding-based retrieval\n",
    "        candidates = self.hybrid_search(query, entitlement, org_id, tags, candidates_for_rerank)\n",
    "        \n",
    "        if not candidates:\n",
    "            return {'query': query, 'documents': [], 'message': 'No documents found'}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[Embedding Search] Top 5:\")\n",
    "            for i, c in enumerate(candidates[:5]):\n",
    "                print(f\"  {i+1}. {c['title']} (hybrid: {c['hybrid_score']:.4f})\")\n",
    "        \n",
    "        # Stage 2: Apply fusion strategy\n",
    "        if fusion_method == 'embedding_only':\n",
    "            candidates = self.fuse_embedding_only(candidates)\n",
    "        else:\n",
    "            # Get re-ranker scores\n",
    "            context_query = self.build_context_query(query, conversation_history)\n",
    "            if verbose and conversation_history:\n",
    "                print(f\"\\n[Context Query]: {context_query}\")\n",
    "            \n",
    "            documents = [c['content'] for c in candidates]\n",
    "            rerank_scores = self.get_rerank_scores(context_query, documents)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n[Re-ranker Scores] Top 5:\")\n",
    "                temp_ranked = sorted(enumerate(rerank_scores), key=lambda x: x[1], reverse=True)\n",
    "                for rank, (i, score) in enumerate(temp_ranked[:5]):\n",
    "                    print(f\"  {rank+1}. {candidates[i]['title']} (rerank: {score:.4f})\")\n",
    "            \n",
    "            # Apply fusion\n",
    "            if fusion_method == 'weighted':\n",
    "                candidates = self.fuse_weighted(candidates, rerank_scores, embedding_weight, reranker_weight)\n",
    "            elif fusion_method == 'rrf':\n",
    "                candidates = self.fuse_rrf(candidates, rerank_scores, rrf_k)\n",
    "            elif fusion_method == 'reranker_only':\n",
    "                candidates = self.fuse_reranker_only(candidates, rerank_scores)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[Final Ranking ({fusion_method})] Top 5:\")\n",
    "            for i, c in enumerate(candidates[:5]):\n",
    "                print(f\"  {i+1}. {c['title']} (final: {c['final_score']:.4f})\")\n",
    "        \n",
    "        # Build response\n",
    "        seen_docs = set()\n",
    "        result_documents = []\n",
    "        \n",
    "        for chunk in candidates:\n",
    "            doc_id = chunk['doc_id']\n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                doc_entry = {\n",
    "                    'document_name': chunk['title'],\n",
    "                    'doc_id': doc_id,\n",
    "                    'final_score': chunk['final_score'],\n",
    "                    'hybrid_score': chunk['hybrid_score'],\n",
    "                }\n",
    "                if 'rerank_score' in chunk:\n",
    "                    doc_entry['rerank_score'] = chunk['rerank_score']\n",
    "                if 'embedding_rank' in chunk:\n",
    "                    doc_entry['embedding_rank'] = chunk['embedding_rank']\n",
    "                    doc_entry['reranker_rank'] = chunk['reranker_rank']\n",
    "                \n",
    "                result_documents.append(doc_entry)\n",
    "                \n",
    "                if len(result_documents) >= top_k:\n",
    "                    break\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'documents': result_documents,\n",
    "            'fusion_method': fusion_method,\n",
    "            'history_used': len(conversation_history) if conversation_history else 0\n",
    "        }\n",
    "    \n",
    "    # Session management\n",
    "    def create_session(self, user_id: str, entitlement: str, org_id: str = None) -> str:\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            'user_id': user_id, 'entitlement': entitlement, 'org_id': org_id, 'query_history': []\n",
    "        }\n",
    "        print(f\"✓ Created session: {session_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def query_with_session(self, session_id: str, query: str, \n",
    "                           fusion_method: str = 'rrf', **kwargs) -> Dict:\n",
    "        session = self.sessions.get(session_id)\n",
    "        if not session:\n",
    "            raise ValueError(f\"Session not found: {session_id}\")\n",
    "        \n",
    "        history = session['query_history'][-3:]\n",
    "        \n",
    "        result = self.query(\n",
    "            query=query,\n",
    "            entitlement=session['entitlement'],\n",
    "            org_id=session['org_id'],\n",
    "            conversation_history=history,\n",
    "            fusion_method=fusion_method,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        session['query_history'].append({\n",
    "            'query': query,\n",
    "            'documents_found': [d['document_name'] for d in result['documents']]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✓ ProductionRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ProductionRetriever(\n",
    "    config=config,\n",
    "    reranker_endpoint=RERANKER_ENDPOINT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Fusion Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: All Fusion Strategies\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"How do I process a cancellation?\"\n",
    "\n",
    "strategies = [\n",
    "    ('embedding_only', 'Embedding Only (no re-ranker)'),\n",
    "    ('reranker_only', 'Re-ranker Only'),\n",
    "    ('weighted', 'Weighted Fusion (60/40)'),\n",
    "    ('rrf', 'Reciprocal Rank Fusion (RRF)'),\n",
    "]\n",
    "\n",
    "for method, label in strategies:\n",
    "    print(f\"\\n{'─'*50}\")\n",
    "    print(f\"{label}\")\n",
    "    print(f\"{'─'*50}\")\n",
    "    \n",
    "    result = retriever.query(\n",
    "        query=test_query,\n",
    "        entitlement='agent_support',\n",
    "        org_id='org_123',\n",
    "        fusion_method=method,\n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    for i, doc in enumerate(result['documents'], 1):\n",
    "        extras = \"\"\n",
    "        if 'rerank_score' in doc:\n",
    "            extras = f\" | rerank={doc['rerank_score']:.3f}\"\n",
    "        if 'embedding_rank' in doc:\n",
    "            extras += f\" | emb_rank={doc['embedding_rank']}, rerank_rank={doc['reranker_rank']}\"\n",
    "        print(f\"  {i}. {doc['document_name']} (final={doc['final_score']:.4f}{extras})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEST: Multi-Turn Conversation with RRF\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "session_id = retriever.create_session('agent_001', 'agent_support', 'org_123')\n",
    "\n",
    "queries = [\n",
    "    \"How do I cancel a booking?\",\n",
    "    \"What about the refund?\",\n",
    "    \"What documents do I need?\"  # Ambiguous - history helps\n",
    "]\n",
    "\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"\\n{'─'*50}\")\n",
    "    print(f\"Turn {i}: {q}\")\n",
    "    print(f\"{'─'*50}\")\n",
    "    \n",
    "    result = retriever.query_with_session(\n",
    "        session_id=session_id,\n",
    "        query=q,\n",
    "        fusion_method='rrf',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTop result: {result['documents'][0]['document_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Which Strategy to Use?\n",
    "\n",
    "| Strategy | Best When | Industry Usage |\n",
    "|----------|-----------|----------------|\n",
    "| **RRF** | General purpose, robust | Elasticsearch, Azure AI Search |\n",
    "| **Weighted** | You know relative signal quality | Pinecone, custom RAG |\n",
    "| **Re-ranker Only** | Re-ranker is domain-tuned | Cohere Rerank |\n",
    "| **Embedding Only** | Low latency needed, re-ranker unreliable | Simple RAG |\n",
    "\n",
    "### Recommendation for Your Case:\n",
    "\n",
    "**Use RRF** - It's the industry standard and handles your situation well:\n",
    "- Doesn't require tuning weights\n",
    "- Robust to close scores (0.93 vs 0.94 won't flip rankings arbitrarily)\n",
    "- Combines rankings, not raw scores\n",
    "\n",
    "```python\n",
    "result = retriever.query(\n",
    "    query=\"...\",\n",
    "    fusion_method='rrf',  # Industry standard\n",
    "    ...\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nRecommended: Use 'rrf' (Reciprocal Rank Fusion)\")\n",
    "print(\"It's the industry standard and handles close scores well.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval with BERT/DistilBERT\n",
    "\n",
    "This notebook replaces LLM-based answer generation with BERT/DistilBERT for semantic similarity-based document retrieval. Instead of generating answers, it returns the document names that contain the relevant information.\n",
    "\n",
    "## Key Changes from Original:\n",
    "- Uses `sentence-transformers` with DistilBERT/BERT for embeddings\n",
    "- No LLM endpoint required\n",
    "- Returns document names instead of generated answers\n",
    "- Lighter weight and faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers faiss-cpu rank_bm25 pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import faiss\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Choose your BERT model variant. Options include:\n",
    "- `all-MiniLM-L6-v2` - Fast, good quality (recommended for most cases)\n",
    "- `all-mpnet-base-v2` - Best quality, slower\n",
    "- `multi-qa-distilbert-cos-v1` - Optimized for Q&A\n",
    "- `distilbert-base-nli-stsb-mean-tokens` - Classic DistilBERT\n",
    "- `bert-base-nli-mean-tokens` - Classic BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Modify as needed\n",
    "CONFIG = {\n",
    "    'model': {\n",
    "        # Choose your model - uncomment one:\n",
    "        'name': 'all-MiniLM-L6-v2',  # Fast and efficient (384 dims)\n",
    "        # 'name': 'all-mpnet-base-v2',  # Higher quality (768 dims)\n",
    "        # 'name': 'multi-qa-distilbert-cos-v1',  # Q&A optimized (768 dims)\n",
    "        # 'name': 'distilbert-base-nli-stsb-mean-tokens',  # DistilBERT (768 dims)\n",
    "        # 'name': 'bert-base-nli-mean-tokens',  # BERT base (768 dims)\n",
    "    },\n",
    "    'retrieval': {\n",
    "        'hybrid': {\n",
    "            'top_k': 5,\n",
    "            'vector_weight': 0.7,\n",
    "            'bm25_weight': 0.3\n",
    "        }\n",
    "    },\n",
    "    'storage': {\n",
    "        'faiss_index': './indexes/faiss',\n",
    "        'bm25_index': './indexes/bm25'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded. Using model: {CONFIG['model']['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT-based Document Retriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDocumentRetriever:\n",
    "    \"\"\"\n",
    "    Document retriever using BERT/DistilBERT for embeddings.\n",
    "    Returns document names instead of generating answers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize BERT/DistilBERT model\n",
    "        model_name = config['model']['name']\n",
    "        print(f\"Loading model: {model_name}...\")\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        print(f\"‚úì Model loaded: {model_name}\")\n",
    "        print(f\"  Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        \n",
    "        # Session management\n",
    "        self.sessions = {}\n",
    "        \n",
    "        # Load indexes\n",
    "        self.load_indexes()\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        \"\"\"Load FAISS and BM25 indexes\"\"\"\n",
    "        # Load FAISS index\n",
    "        faiss_path = os.path.join(self.config['storage']['faiss_index'], 'faiss.index')\n",
    "        if not os.path.exists(faiss_path):\n",
    "            raise FileNotFoundError(f\"FAISS index not found at {faiss_path}. Run indexing notebook first.\")\n",
    "        \n",
    "        self.faiss_index = faiss.read_index(faiss_path)\n",
    "        print(f\"‚úì FAISS index loaded: {self.faiss_index.ntotal} vectors\")\n",
    "        \n",
    "        # Load embeddings (optional)\n",
    "        embeddings_path = os.path.join(self.config['storage']['faiss_index'], 'embeddings.npy')\n",
    "        if os.path.exists(embeddings_path):\n",
    "            self.embeddings = np.load(embeddings_path)\n",
    "            print(f\"‚úì Embeddings loaded: shape {self.embeddings.shape}\")\n",
    "        \n",
    "        # Load BM25 index\n",
    "        bm25_path = os.path.join(self.config['storage']['bm25_index'], 'bm25.pkl')\n",
    "        if not os.path.exists(bm25_path):\n",
    "            raise FileNotFoundError(f\"BM25 index not found at {bm25_path}. Run indexing notebook first.\")\n",
    "        \n",
    "        with open(bm25_path, 'rb') as f:\n",
    "            self.bm25_index = pickle.load(f)\n",
    "        print(f\"‚úì BM25 index loaded\")\n",
    "        \n",
    "        # Load chunk metadata\n",
    "        metadata_path = os.path.join(self.config['storage']['faiss_index'], 'chunk_metadata.json')\n",
    "        if not os.path.exists(metadata_path):\n",
    "            raise FileNotFoundError(f\"Chunk metadata not found at {metadata_path}. Run indexing notebook first.\")\n",
    "        \n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        print(f\"‚úì Chunk metadata loaded: {len(self.chunks)} chunks\")\n",
    "        \n",
    "        print(\"\\n‚úì All indexes loaded successfully\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get embedding using BERT/DistilBERT\"\"\"\n",
    "        embedding = self.model.encode(text, convert_to_numpy=True)\n",
    "        return embedding.astype('float32')\n",
    "    \n",
    "    def hybrid_search(self, query: str, entitlement: str, org_id: str = None,\n",
    "                      tags: List[str] = None, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search with filtering\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config['retrieval']['hybrid']['top_k']\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Retrieve more results initially for filtering\n",
    "        retrieval_multiplier = 10\n",
    "        initial_top_k = min(top_k * retrieval_multiplier, len(self.chunks))\n",
    "        \n",
    "        # Vector search (FAISS)\n",
    "        vector_scores, vector_indices = self.faiss_index.search(query_embedding, initial_top_k)\n",
    "        vector_scores = vector_scores[0]\n",
    "        vector_indices = vector_indices[0]\n",
    "        \n",
    "        # Keyword search (BM25)\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize scores\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        vector_scores_norm = normalize(vector_scores)\n",
    "        bm25_scores_norm = normalize(bm25_scores)\n",
    "        \n",
    "        # Compute hybrid scores\n",
    "        vector_weight = self.config['retrieval']['hybrid']['vector_weight']\n",
    "        bm25_weight = self.config['retrieval']['hybrid']['bm25_weight']\n",
    "        \n",
    "        hybrid_scores = {}\n",
    "        for idx, score in zip(vector_indices, vector_scores_norm):\n",
    "            hybrid_scores[idx] = score * vector_weight\n",
    "        \n",
    "        for idx, score in enumerate(bm25_scores_norm):\n",
    "            if idx in hybrid_scores:\n",
    "                hybrid_scores[idx] += score * bm25_weight\n",
    "            else:\n",
    "                hybrid_scores[idx] = score * bm25_weight\n",
    "        \n",
    "        # Sort by score\n",
    "        sorted_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Filter and collect results\n",
    "        accessible_results = []\n",
    "        \n",
    "        for idx, score in sorted_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            \n",
    "            # Apply entitlement filter\n",
    "            chunk_entitlements = chunk.get('entitlement', ['universal'])\n",
    "            if isinstance(chunk_entitlements, str):\n",
    "                chunk_entitlements = [chunk_entitlements]\n",
    "            \n",
    "            has_access = (\n",
    "                'universal' in chunk_entitlements or\n",
    "                entitlement in chunk_entitlements\n",
    "            )\n",
    "            \n",
    "            if not has_access:\n",
    "                continue\n",
    "            \n",
    "            # Apply org filter\n",
    "            if org_id and chunk.get('orgId') != org_id:\n",
    "                continue\n",
    "            \n",
    "            # Apply tag filter\n",
    "            if tags and not any(t in chunk.get('metadata', {}).get('tags', []) for t in tags):\n",
    "                continue\n",
    "            \n",
    "            chunk['score'] = float(score)\n",
    "            accessible_results.append(chunk)\n",
    "        \n",
    "        # Sort and return top K\n",
    "        accessible_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "        return accessible_results[:top_k]\n",
    "    \n",
    "    def get_relevant_documents(self, query: str, entitlement: str, \n",
    "                               org_id: str = None, tags: List[str] = None,\n",
    "                               top_k: int = 5) -> Dict:\n",
    "        \"\"\"\n",
    "        Main retrieval function - returns document names instead of generated answers.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with 'documents' list containing document info and 'query' string\n",
    "        \"\"\"\n",
    "        chunks = self.hybrid_search(query, entitlement, org_id=org_id, tags=tags, top_k=top_k)\n",
    "        \n",
    "        if not chunks:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'documents': [],\n",
    "                'message': 'No relevant documents found.'\n",
    "            }\n",
    "        \n",
    "        # Extract unique documents (deduplicate by doc_id)\n",
    "        seen_docs = set()\n",
    "        documents = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            doc_id = chunk.get('doc_id', chunk.get('title', 'Unknown'))\n",
    "            \n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                documents.append({\n",
    "                    'document_name': chunk.get('title', 'Unknown'),\n",
    "                    'doc_id': doc_id,\n",
    "                    'score': chunk['score'],\n",
    "                    'chunk_preview': chunk.get('content', '')[:200] + '...',\n",
    "                    'metadata': chunk.get('metadata', {})\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'documents': documents,\n",
    "            'total_chunks_found': len(chunks)\n",
    "        }\n",
    "    \n",
    "    # Session management methods\n",
    "    def create_session(self, user_id: str, entitlement: str, org_id: str = None) -> str:\n",
    "        \"\"\"Create a new session\"\"\"\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            'session_id': session_id,\n",
    "            'user_id': user_id,\n",
    "            'entitlement': entitlement,\n",
    "            'org_id': org_id,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'query_history': [],\n",
    "            'last_activity': datetime.now().isoformat()\n",
    "        }\n",
    "        print(f\"‚úì Created session: {session_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def query_with_session(self, session_id: str, query: str,\n",
    "                           tags: List[str] = None, top_k: int = 5) -> Dict:\n",
    "        \"\"\"Query with session context\"\"\"\n",
    "        session = self.sessions.get(session_id)\n",
    "        if not session:\n",
    "            raise ValueError(f\"Session {session_id} not found\")\n",
    "        \n",
    "        session['last_activity'] = datetime.now().isoformat()\n",
    "        \n",
    "        result = self.get_relevant_documents(\n",
    "            query,\n",
    "            session['entitlement'],\n",
    "            org_id=session['org_id'],\n",
    "            tags=tags,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        result['session_id'] = session_id\n",
    "        \n",
    "        # Store in history\n",
    "        session['query_history'].append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'query': query,\n",
    "            'documents_found': [d['document_name'] for d in result['documents']]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> List[Dict]:\n",
    "        \"\"\"Get query history for a session\"\"\"\n",
    "        session = self.sessions.get(session_id)\n",
    "        return session['query_history'] if session else []\n",
    "\n",
    "print(\"‚úì BERTDocumentRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alternative: Create Index with BERT Embeddings\n",
    "\n",
    "If you need to create new indexes using BERT embeddings (instead of using existing indexes from the original pipeline), use this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "def create_bert_indexes(documents: List[Dict], model_name: str = 'all-MiniLM-L6-v2',\n",
    "                        output_dir: str = './indexes') -> None:\n",
    "    \"\"\"\n",
    "    Create FAISS and BM25 indexes from documents using BERT embeddings.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of dicts with 'content', 'title', 'doc_id', etc.\n",
    "        model_name: Sentence transformer model name\n",
    "        output_dir: Directory to save indexes\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    print(f\"Loading model: {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Create output directories\n",
    "    faiss_dir = os.path.join(output_dir, 'faiss')\n",
    "    bm25_dir = os.path.join(output_dir, 'bm25')\n",
    "    os.makedirs(faiss_dir, exist_ok=True)\n",
    "    os.makedirs(bm25_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(f\"Generating embeddings for {len(documents)} documents...\")\n",
    "    texts = [doc['content'] for doc in documents]\n",
    "    embeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    embeddings = embeddings.astype('float32')\n",
    "    \n",
    "    # Normalize for cosine similarity\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # Inner product for normalized vectors = cosine similarity\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    # Save FAISS index and embeddings\n",
    "    faiss.write_index(index, os.path.join(faiss_dir, 'faiss.index'))\n",
    "    np.save(os.path.join(faiss_dir, 'embeddings.npy'), embeddings)\n",
    "    print(f\"‚úì FAISS index saved: {index.ntotal} vectors\")\n",
    "    \n",
    "    # Save chunk metadata\n",
    "    with open(os.path.join(faiss_dir, 'chunk_metadata.json'), 'w') as f:\n",
    "        json.dump(documents, f, indent=2)\n",
    "    print(f\"‚úì Chunk metadata saved\")\n",
    "    \n",
    "    # Create BM25 index\n",
    "    tokenized_docs = [doc['content'].lower().split() for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    with open(os.path.join(bm25_dir, 'bm25.pkl'), 'wb') as f:\n",
    "        pickle.dump(bm25, f)\n",
    "    print(f\"‚úì BM25 index saved\")\n",
    "    \n",
    "    print(f\"\\n‚úì All indexes created successfully in {output_dir}\")\n",
    "\n",
    "print(\"‚úì Index creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: Create Sample Documents and Index\n",
    "\n",
    "If you don't have existing indexes, run this cell to create sample data and indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for demonstration\n",
    "SAMPLE_DOCUMENTS = [\n",
    "    {\n",
    "        'doc_id': 'doc_001',\n",
    "        'title': 'Cancellation Policy Guide',\n",
    "        'content': 'To process a cancellation, first verify the customer identity. Then check the booking status in the system. If eligible for refund, initiate the cancellation workflow and process the refund within 5-7 business days.',\n",
    "        'entitlement': ['agent_support', 'universal'],\n",
    "        'orgId': 'org_123',\n",
    "        'metadata': {'tags': ['cancellation', 'refund', 'policy']}\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'doc_002',\n",
    "        'title': 'Booking Creation Process',\n",
    "        'content': 'To create a new booking, collect customer details including name, contact, and travel dates. Search for availability in the system. Select the appropriate option and confirm with payment.',\n",
    "        'entitlement': ['agent_sales', 'universal'],\n",
    "        'orgId': 'org_123',\n",
    "        'metadata': {'tags': ['booking', 'sales']}\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'doc_003',\n",
    "        'title': 'Refund Processing Guidelines',\n",
    "        'content': 'Refunds must be processed within 48 hours of approval. Required documents include original receipt, cancellation confirmation, and customer ID. Refunds are issued to the original payment method.',\n",
    "        'entitlement': ['agent_support'],\n",
    "        'orgId': 'org_123',\n",
    "        'metadata': {'tags': ['refund', 'cancellation']}\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'doc_004',\n",
    "        'title': 'Customer Verification Procedures',\n",
    "        'content': 'Always verify customer identity using two-factor authentication. Check government ID and booking reference number. For sensitive operations, additional security questions may be required.',\n",
    "        'entitlement': ['universal'],\n",
    "        'orgId': 'org_123',\n",
    "        'metadata': {'tags': ['security', 'verification']}\n",
    "    },\n",
    "    {\n",
    "        'doc_id': 'doc_005',\n",
    "        'title': 'Sales Commission Structure',\n",
    "        'content': 'Sales agents earn 5% commission on standard bookings and 7% on premium packages. Commissions are calculated monthly and paid on the 15th of each month.',\n",
    "        'entitlement': ['agent_sales', 'agent_manager'],\n",
    "        'orgId': 'org_123',\n",
    "        'metadata': {'tags': ['sales', 'commission']}\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create indexes\n",
    "create_bert_indexes(SAMPLE_DOCUMENTS, model_name=CONFIG['model']['name'])\n",
    "\n",
    "print(\"\\n‚úì Sample indexes created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT-based retriever\n",
    "retriever = BERTDocumentRetriever(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Document Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Simple query\n",
    "print(\"=\"*70)\n",
    "print(\"TEST 1: Simple Document Retrieval\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query = \"How do I process a cancellation?\"\n",
    "result = retriever.get_relevant_documents(\n",
    "    query=query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {result['query']}\")\n",
    "print(f\"\\nRelevant Documents Found:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for i, doc in enumerate(result['documents'], 1):\n",
    "    print(f\"\\n{i}. {doc['document_name']}\")\n",
    "    print(f\"   Doc ID: {doc['doc_id']}\")\n",
    "    print(f\"   Score: {doc['score']:.4f}\")\n",
    "    print(f\"   Preview: {doc['chunk_preview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Query with session\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: Document Retrieval with Session\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create session\n",
    "session_id = retriever.create_session(\n",
    "    user_id='agent_001',\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "# Query 1\n",
    "print(\"\\n--- Query 1 ---\")\n",
    "result1 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"What documents do I need for a refund?\"\n",
    ")\n",
    "\n",
    "print(f\"Query: {result1['query']}\")\n",
    "print(f\"Documents found: {[d['document_name'] for d in result1['documents']]}\")\n",
    "\n",
    "# Query 2\n",
    "print(\"\\n--- Query 2 ---\")\n",
    "result2 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"How do I verify customer identity?\"\n",
    ")\n",
    "\n",
    "print(f\"Query: {result2['query']}\")\n",
    "print(f\"Documents found: {[d['document_name'] for d in result2['documents']]}\")\n",
    "\n",
    "# Show session history\n",
    "print(\"\\n--- Session History ---\")\n",
    "history = retriever.get_session_history(session_id)\n",
    "for item in history:\n",
    "    print(f\"  [{item['timestamp'][:19]}] {item['query']}\")\n",
    "    print(f\"    ‚Üí Found: {item['documents_found']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Entitlement-based filtering\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: Entitlement-Based Access Control\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "same_query = \"What are the sales commission rates?\"\n",
    "\n",
    "# Support agent (should not see sales commission doc)\n",
    "print(\"\\n--- Support Agent ---\")\n",
    "support_result = retriever.get_relevant_documents(\n",
    "    query=same_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "print(f\"Query: {same_query}\")\n",
    "print(f\"Documents found: {[d['document_name'] for d in support_result['documents']]}\")\n",
    "\n",
    "# Sales agent (should see sales commission doc)\n",
    "print(\"\\n--- Sales Agent ---\")\n",
    "sales_result = retriever.get_relevant_documents(\n",
    "    query=same_query,\n",
    "    entitlement='agent_sales',\n",
    "    org_id='org_123'\n",
    ")\n",
    "print(f\"Query: {same_query}\")\n",
    "print(f\"Documents found: {[d['document_name'] for d in sales_result['documents']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Utility Function: Pretty Print Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(result: Dict, show_preview: bool = True):\n",
    "    \"\"\"\n",
    "    Pretty print retrieval results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not result['documents']:\n",
    "        print(\"\\n‚ùå No relevant documents found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n‚úì Found {len(result['documents'])} relevant document(s):\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for i, doc in enumerate(result['documents'], 1):\n",
    "        print(f\"\\nüìÑ {i}. {doc['document_name']}\")\n",
    "        print(f\"   Relevance Score: {doc['score']:.4f}\")\n",
    "        print(f\"   Document ID: {doc['doc_id']}\")\n",
    "        \n",
    "        if show_preview:\n",
    "            print(f\"   Preview: {doc['chunk_preview'][:150]}...\")\n",
    "        \n",
    "        if doc.get('metadata', {}).get('tags'):\n",
    "            print(f\"   Tags: {', '.join(doc['metadata']['tags'])}\")\n",
    "\n",
    "# Example usage\n",
    "result = retriever.get_relevant_documents(\n",
    "    query=\"How do I create a new booking?\",\n",
    "    entitlement='agent_sales',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "display_results(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Query Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_query_mode(retriever, entitlement: str = 'agent_support', org_id: str = 'org_123'):\n",
    "    \"\"\"\n",
    "    Interactive mode to query documents.\n",
    "    Type 'quit' to exit.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìö Interactive Document Retrieval\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Entitlement: {entitlement}\")\n",
    "    print(f\"Organization: {org_id}\")\n",
    "    print(\"Type 'quit' to exit.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"\\nüîç Enter your query: \").strip()\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"\\nExiting interactive mode. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query:\n",
    "            print(\"Please enter a valid query.\")\n",
    "            continue\n",
    "        \n",
    "        result = retriever.get_relevant_documents(\n",
    "            query=query,\n",
    "            entitlement=entitlement,\n",
    "            org_id=org_id\n",
    "        )\n",
    "        \n",
    "        display_results(result)\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_query_mode(retriever, entitlement='agent_support', org_id='org_123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Differences from LLM Approach:\n",
    "\n",
    "| Feature | LLM Approach | BERT Approach |\n",
    "|---------|--------------|---------------|\n",
    "| Output | Generated answer text | Document names/IDs |\n",
    "| Model | Large LLM (Llama, etc.) | BERT/DistilBERT |\n",
    "| Compute | High (LLM inference) | Low (embedding only) |\n",
    "| Latency | Higher | Lower |\n",
    "| Cost | Higher (LLM endpoint) | Lower (local/smaller model) |\n",
    "| Use Case | Q&A, chatbots | Document search, retrieval |\n",
    "\n",
    "### Available Models:\n",
    "\n",
    "- `all-MiniLM-L6-v2` - Fast, 384 dimensions\n",
    "- `all-mpnet-base-v2` - Best quality, 768 dimensions\n",
    "- `multi-qa-distilbert-cos-v1` - Q&A optimized, 768 dimensions\n",
    "- `distilbert-base-nli-stsb-mean-tokens` - DistilBERT, 768 dimensions\n",
    "- `bert-base-nli-mean-tokens` - BERT, 768 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nThe BERTDocumentRetriever is ready to use!\")\n",
    "print(\"\\nKey methods:\")\n",
    "print(\"  - get_relevant_documents(query, entitlement, ...) ‚Üí Returns document names\")\n",
    "print(\"  - create_session(user_id, entitlement, ...) ‚Üí Creates a session\")\n",
    "print(\"  - query_with_session(session_id, query, ...) ‚Üí Query with session tracking\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

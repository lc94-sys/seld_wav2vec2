{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-Reranker with Correct Response Parsing\n",
    "\n",
    "Updated to parse the response format:\n",
    "```json\n",
    "{\n",
    "    \"id\": \"\",\n",
    "    \"model\": \"\",\n",
    "    \"usage\": {\"total_tokens\": \"\"},\n",
    "    \"results\": [\n",
    "        {\"index\": 0, \"document\": {...}, \"relevance_score\": 0.95},\n",
    "        {\"index\": 1, \"document\": {...}, \"relevance_score\": 0.72}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import faiss\n",
    "import boto3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# Load configuration\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RERANKER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "RERANKER_CONFIG = {\n",
    "    'endpoint_name': 'your-qwen3-reranker-endpoint',  # <-- UPDATE THIS\n",
    "    'region': config['models']['embedding']['credentials']['region'],\n",
    "    'credentials': {\n",
    "        'accessKeyId': config['models']['embedding']['credentials']['accessKeyId'],\n",
    "        'secretAccessKey': config['models']['embedding']['credentials']['secretAccessKey'],\n",
    "        'sessionToken': config['models']['embedding']['credentials']['sessionToken']\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Reranker endpoint: {RERANKER_CONFIG['endpoint_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3RerankerClient:\n",
    "    \"\"\"\n",
    "    Client for Qwen3-Reranker-8B API.\n",
    "    \n",
    "    Expected response format:\n",
    "    {\n",
    "        \"results\": [\n",
    "            {\"index\": 0, \"document\": {...}, \"relevance_score\": 0.95},\n",
    "            {\"index\": 1, \"document\": {...}, \"relevance_score\": 0.72}\n",
    "        ]\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.endpoint_name = config['endpoint_name']\n",
    "        creds = config['credentials']\n",
    "        \n",
    "        self.client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=config['region'],\n",
    "            aws_access_key_id=creds['accessKeyId'],\n",
    "            aws_secret_access_key=creds['secretAccessKey'],\n",
    "            aws_session_token=creds['sessionToken']\n",
    "        )\n",
    "        print(f\"✓ Qwen3-Reranker client initialized\")\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], verbose: bool = False) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get relevance scores for query-document pairs.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            documents: List of document contents\n",
    "            verbose: Print debug info\n",
    "        \n",
    "        Returns:\n",
    "            List of relevance scores (higher = more relevant)\n",
    "        \"\"\"\n",
    "        # Build request payload\n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"documents\": documents\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[DEBUG] Query: {query[:100]}...\")\n",
    "            print(f\"[DEBUG] Number of documents: {len(documents)}\")\n",
    "        \n",
    "        # Call API\n",
    "        response = self.client.invoke_endpoint(\n",
    "            EndpointName=self.endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "        \n",
    "        # Read response body ONCE\n",
    "        raw_bytes = response['Body'].read()\n",
    "        output_data = json.loads(raw_bytes.decode('utf-8'))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[DEBUG] Response keys: {output_data.keys()}\")\n",
    "            print(f\"[DEBUG] Number of results: {len(output_data.get('results', []))}\")\n",
    "        \n",
    "        # =============================================================\n",
    "        # PARSE SCORES FROM YOUR RESPONSE FORMAT\n",
    "        # =============================================================\n",
    "        results = output_data['results']\n",
    "        \n",
    "        # Extract relevance_score from each result\n",
    "        # Results might not be in order, so we sort by index\n",
    "        scores_with_index = [\n",
    "            (result['index'], result['relevance_score']) \n",
    "            for result in results\n",
    "        ]\n",
    "        \n",
    "        # Sort by index to match original document order\n",
    "        scores_with_index.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Extract just the scores in order\n",
    "        scores = [score for idx, score in scores_with_index]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[DEBUG] Extracted scores: {scores}\")\n",
    "        \n",
    "        return scores\n",
    "\n",
    "print(\"✓ Qwen3RerankerClient class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen3ReRankingRetriever:\n",
    "    \"\"\"\n",
    "    Document retriever with Qwen3-Reranker-8B.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, reranker_config: dict):\n",
    "        self.config = config\n",
    "        \n",
    "        # Initialize Qwen Embedding Client\n",
    "        self.embedding_endpoint_name = config['models']['embedding']['endpoint_name']\n",
    "        embedding_creds = config['models']['embedding']['credentials']\n",
    "        self.embedding_client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=embedding_creds['region'],\n",
    "            aws_access_key_id=embedding_creds['accessKeyId'],\n",
    "            aws_secret_access_key=embedding_creds['secretAccessKey'],\n",
    "            aws_session_token=embedding_creds['sessionToken']\n",
    "        )\n",
    "        print(f\"✓ Qwen Embedding client initialized\")\n",
    "        \n",
    "        # Initialize Qwen3-Reranker Client\n",
    "        self.reranker = Qwen3RerankerClient(reranker_config)\n",
    "        \n",
    "        # Session management\n",
    "        self.sessions = {}\n",
    "        \n",
    "        # Load indexes\n",
    "        self.load_indexes()\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        \"\"\"Load FAISS and BM25 indexes\"\"\"\n",
    "        faiss_path = os.path.join(self.config['storage']['faiss_index'], 'faiss.index')\n",
    "        self.faiss_index = faiss.read_index(faiss_path)\n",
    "        print(f\"✓ FAISS index loaded\")\n",
    "        \n",
    "        embeddings_path = os.path.join(self.config['storage']['faiss_index'], 'embeddings.npy')\n",
    "        if os.path.exists(embeddings_path):\n",
    "            self.embeddings = np.load(embeddings_path)\n",
    "        \n",
    "        bm25_path = os.path.join(self.config['storage']['bm25_index'], 'bm25.pkl')\n",
    "        with open(bm25_path, 'rb') as f:\n",
    "            self.bm25_index = pickle.load(f)\n",
    "        print(f\"✓ BM25 index loaded\")\n",
    "        \n",
    "        metadata_path = os.path.join(self.config['storage']['faiss_index'], 'chunk_metadata.json')\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        print(f\"✓ Chunk metadata loaded: {len(self.chunks)} chunks\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get embedding from Qwen endpoint\"\"\"\n",
    "        params = {\"inputs\": [text], \"encoding_format\": \"float\"}\n",
    "        response = self.embedding_client.invoke_endpoint(\n",
    "            EndpointName=self.embedding_endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(params)\n",
    "        )\n",
    "        raw_bytes = response['Body'].read()\n",
    "        output_data = json.loads(raw_bytes.decode())\n",
    "        return np.array(output_data[0], dtype='float32')\n",
    "    \n",
    "    def hybrid_search(self, query: str, entitlement: str, org_id: str = None,\n",
    "                      tags: List[str] = None, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"Stage 1: Hybrid search\"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config['retrieval']['hybrid']['top_k']\n",
    "        \n",
    "        query_embedding = self.get_embedding(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        initial_top_k = min(top_k * 10, len(self.chunks))\n",
    "        \n",
    "        # Vector search\n",
    "        vector_scores, vector_indices = self.faiss_index.search(query_embedding, initial_top_k)\n",
    "        vector_scores = vector_scores[0]\n",
    "        vector_indices = vector_indices[0]\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        vector_scores_norm = normalize(vector_scores)\n",
    "        bm25_scores_norm = normalize(bm25_scores)\n",
    "        \n",
    "        # Hybrid scores\n",
    "        vector_weight = self.config['retrieval']['hybrid']['vector_weight']\n",
    "        bm25_weight = self.config['retrieval']['hybrid']['bm25_weight']\n",
    "        \n",
    "        hybrid_scores = {}\n",
    "        for idx, score in zip(vector_indices, vector_scores_norm):\n",
    "            hybrid_scores[idx] = score * vector_weight\n",
    "        \n",
    "        for idx, score in enumerate(bm25_scores_norm):\n",
    "            if idx in hybrid_scores:\n",
    "                hybrid_scores[idx] += score * bm25_weight\n",
    "            else:\n",
    "                hybrid_scores[idx] = score * bm25_weight\n",
    "        \n",
    "        sorted_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Filter by access\n",
    "        accessible_results = []\n",
    "        for idx, score in sorted_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            \n",
    "            chunk_entitlements = chunk['entitlement']\n",
    "            if isinstance(chunk_entitlements, str):\n",
    "                chunk_entitlements = [chunk_entitlements]\n",
    "            \n",
    "            has_access = 'universal' in chunk_entitlements or entitlement in chunk_entitlements\n",
    "            if not has_access:\n",
    "                continue\n",
    "            \n",
    "            if org_id and chunk['orgId'] != org_id:\n",
    "                continue\n",
    "            \n",
    "            if tags and not any(t in chunk['metadata']['tags'] for t in tags):\n",
    "                continue\n",
    "            \n",
    "            chunk['hybrid_score'] = float(score)\n",
    "            accessible_results.append(chunk)\n",
    "        \n",
    "        accessible_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return accessible_results[:top_k]\n",
    "    \n",
    "    def rerank(self, query: str, candidates: List[Dict], top_k: int = 5,\n",
    "               verbose: bool = False) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Stage 2: Re-rank using Qwen3-Reranker-8B.\n",
    "        Higher relevance_score = more relevant.\n",
    "        \"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        # Extract document contents\n",
    "        documents = [chunk['content'] for chunk in candidates]\n",
    "        \n",
    "        # Get rerank scores\n",
    "        rerank_scores = self.reranker.rerank(query, documents, verbose=verbose)\n",
    "        \n",
    "        # Add scores to candidates\n",
    "        for i, chunk in enumerate(candidates):\n",
    "            chunk['rerank_score'] = rerank_scores[i]\n",
    "        \n",
    "        # Sort by rerank score (HIGHER = better for relevance_score)\n",
    "        reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        return reranked[:top_k]\n",
    "    \n",
    "    def query(self, query: str, entitlement: str, org_id: str = None,\n",
    "              tags: List[str] = None, top_k: int = 5,\n",
    "              candidates_for_rerank: int = 20,\n",
    "              use_reranker: bool = True,\n",
    "              verbose: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Full retrieval pipeline.\n",
    "        \"\"\"\n",
    "        # Stage 1: Hybrid search\n",
    "        candidates = self.hybrid_search(\n",
    "            query=query,\n",
    "            entitlement=entitlement,\n",
    "            org_id=org_id,\n",
    "            tags=tags,\n",
    "            top_k=candidates_for_rerank\n",
    "        )\n",
    "        \n",
    "        if not candidates:\n",
    "            return {'query': query, 'documents': [], 'message': 'No relevant documents found.'}\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[HYBRID SEARCH] Top candidates:\")\n",
    "            for i, c in enumerate(candidates[:5]):\n",
    "                print(f\"  {i+1}. {c['title']} (hybrid: {c['hybrid_score']:.4f})\")\n",
    "        \n",
    "        # Stage 2: Re-rank\n",
    "        if use_reranker:\n",
    "            final_results = self.rerank(query, candidates, top_k=top_k, verbose=verbose)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"\\n[RE-RANKED] Final results:\")\n",
    "                for i, c in enumerate(final_results):\n",
    "                    print(f\"  {i+1}. {c['title']} (rerank: {c['rerank_score']:.4f}, hybrid: {c['hybrid_score']:.4f})\")\n",
    "        else:\n",
    "            final_results = candidates[:top_k]\n",
    "        \n",
    "        # Build response\n",
    "        seen_docs = set()\n",
    "        documents = []\n",
    "        for chunk in final_results:\n",
    "            doc_id = chunk['doc_id']\n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                doc_entry = {\n",
    "                    'document_name': chunk['title'],\n",
    "                    'doc_id': doc_id,\n",
    "                    'hybrid_score': chunk['hybrid_score'],\n",
    "                }\n",
    "                if use_reranker:\n",
    "                    doc_entry['rerank_score'] = chunk['rerank_score']\n",
    "                documents.append(doc_entry)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'documents': documents,\n",
    "            'reranker_used': use_reranker\n",
    "        }\n",
    "    \n",
    "    # Session management\n",
    "    def create_session(self, user_id: str, entitlement: str, org_id: str = None) -> str:\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            'session_id': session_id,\n",
    "            'user_id': user_id,\n",
    "            'entitlement': entitlement,\n",
    "            'org_id': org_id,\n",
    "            'query_history': []\n",
    "        }\n",
    "        print(f\"✓ Created session: {session_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def query_with_session(self, session_id: str, query: str, **kwargs) -> Dict:\n",
    "        session = self.sessions.get(session_id)\n",
    "        if not session:\n",
    "            raise ValueError(f\"Session {session_id} not found\")\n",
    "        \n",
    "        result = self.query(\n",
    "            query=query,\n",
    "            entitlement=session['entitlement'],\n",
    "            org_id=session['org_id'],\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        session['query_history'].append({\n",
    "            'query': query,\n",
    "            'documents_found': [d['document_name'] for d in result['documents']]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"✓ Qwen3ReRankingRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retriever\n",
    "retriever = Qwen3ReRankingRetriever(\n",
    "    config=config,\n",
    "    reranker_config=RERANKER_CONFIG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with verbose output\n",
    "print(\"=\"*70)\n",
    "print(\"TEST: Query with Re-Ranking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = retriever.query(\n",
    "    query='How do I process a cancellation?',\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    top_k=5,\n",
    "    candidates_for_rerank=20,\n",
    "    use_reranker=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result['documents'], 1):\n",
    "    print(f\"{i}. {doc['document_name']}\")\n",
    "    print(f\"   Rerank Score: {doc.get('rerank_score', 'N/A')}\")\n",
    "    print(f\"   Hybrid Score: {doc['hybrid_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with and without re-ranker\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: Hybrid Only vs Re-Ranked\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"How do I process a cancellation?\"\n",
    "\n",
    "# Without re-ranker\n",
    "print(\"\\n--- WITHOUT RE-RANKER (Hybrid Only) ---\")\n",
    "result_hybrid = retriever.query(\n",
    "    query=test_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    use_reranker=False\n",
    ")\n",
    "for i, doc in enumerate(result_hybrid['documents'], 1):\n",
    "    print(f\"  {i}. {doc['document_name']} (hybrid: {doc['hybrid_score']:.4f})\")\n",
    "\n",
    "# With re-ranker\n",
    "print(\"\\n--- WITH RE-RANKER ---\")\n",
    "result_rerank = retriever.query(\n",
    "    query=test_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    use_reranker=True\n",
    ")\n",
    "for i, doc in enumerate(result_rerank['documents'], 1):\n",
    "    print(f\"  {i}. {doc['document_name']} (rerank: {doc['rerank_score']:.4f}, hybrid: {doc['hybrid_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Re-Ranker Still Gives Wrong Order\n",
    "\n",
    "If the document with highest hybrid score is still ranked lower by re-ranker, check:\n",
    "\n",
    "1. **Are the relevance_scores what you expect?** (Higher should be better)\n",
    "2. **Is the input format correct?** Try `pairs` format instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test re-ranker directly with known documents\n",
    "print(\"=\"*70)\n",
    "print(\"DIRECT RE-RANKER TEST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"How do I cancel a booking?\"\n",
    "test_docs = [\n",
    "    \"To cancel a booking, verify customer identity first, then process the cancellation in the system.\",  # Most relevant\n",
    "    \"Refunds are processed within 5-7 business days after cancellation.\",  # Somewhat relevant\n",
    "    \"To create a new booking, enter customer details and payment information.\"  # Not relevant\n",
    "]\n",
    "\n",
    "scores = retriever.reranker.rerank(test_query, test_docs, verbose=True)\n",
    "\n",
    "print(f\"\\nScores:\")\n",
    "for i, (doc, score) in enumerate(zip(test_docs, scores)):\n",
    "    print(f\"  Doc {i}: score={score:.4f} - {doc[:50]}...\")\n",
    "\n",
    "print(f\"\\nExpected: Doc 0 should have highest score (most relevant to cancellation)\")\n",
    "print(f\"Actual highest: Doc {scores.index(max(scores))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

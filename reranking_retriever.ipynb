{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval with Re-Ranking Model\n",
    "\n",
    "This notebook replaces the Llama LLM with a **re-ranking model (cross-encoder)** for document retrieval.\n",
    "\n",
    "## How Re-Ranking Works:\n",
    "- The re-ranker receives the **full document content** (just like the LLM did)\n",
    "- It processes query + document text **together** to compute a relevance score\n",
    "- This is more accurate than embedding similarity because it captures cross-attention between query and document\n",
    "\n",
    "## Pipeline:\n",
    "```\n",
    "Query â†’ Hybrid Search (Qwen + BM25) â†’ Candidate Chunks (with full content) â†’ Re-Ranker â†’ Ranked Documents\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import faiss\n",
    "import boto3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"âœ“ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose Re-Ranking Model\n",
    "\n",
    "| Model | Speed | Quality | Notes |\n",
    "|-------|-------|---------|-------|\n",
    "| `cross-encoder/ms-marco-MiniLM-L-6-v2` | âš¡âš¡âš¡ | â­â­â­ | Fast, good for most cases |\n",
    "| `cross-encoder/ms-marco-MiniLM-L-12-v2` | âš¡âš¡ | â­â­â­â­ | Balanced |\n",
    "| `BAAI/bge-reranker-base` | âš¡âš¡ | â­â­â­â­ | High quality |\n",
    "| `BAAI/bge-reranker-large` | âš¡ | â­â­â­â­â­ | Best quality |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your re-ranking model\n",
    "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-6-v2'\n",
    "# RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-12-v2'\n",
    "# RERANKER_MODEL = 'BAAI/bge-reranker-base'\n",
    "# RERANKER_MODEL = 'BAAI/bge-reranker-large'\n",
    "\n",
    "print(f\"Selected re-ranker: {RERANKER_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ReRankingRetriever Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReRankingRetriever:\n",
    "    \"\"\"\n",
    "    Two-stage document retriever:\n",
    "    \n",
    "    Stage 1: Hybrid search (Qwen embeddings + BM25) to get candidate chunks\n",
    "    Stage 2: Cross-encoder re-ranker scores query against FULL document content\n",
    "    \n",
    "    The re-ranker receives the complete chunk content, just like the LLM did.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, reranker_model: str):\n",
    "        self.config = config\n",
    "        \n",
    "        # ============================================================\n",
    "        # Initialize Qwen Embedding Client (UNCHANGED from original)\n",
    "        # ============================================================\n",
    "        self.embedding_endpoint_name = config['models']['embedding']['endpoint_name']\n",
    "        embedding_creds = config['models']['embedding']['credentials']\n",
    "        self.embedding_client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=embedding_creds['region'],\n",
    "            aws_access_key_id=embedding_creds['accessKeyId'],\n",
    "            aws_secret_access_key=embedding_creds['secretAccessKey'],\n",
    "            aws_session_token=embedding_creds['sessionToken']\n",
    "        )\n",
    "        print(f\"âœ“ Qwen embedding client initialized\")\n",
    "        \n",
    "        # ============================================================\n",
    "        # Initialize Re-Ranking Model (REPLACES Llama LLM)\n",
    "        # ============================================================\n",
    "        print(f\"Loading re-ranker: {reranker_model}...\")\n",
    "        self.reranker = CrossEncoder(reranker_model, max_length=512)\n",
    "        self.reranker_model_name = reranker_model\n",
    "        print(f\"âœ“ Re-ranker loaded: {reranker_model}\")\n",
    "        \n",
    "        # Session management\n",
    "        self.sessions = {}\n",
    "        \n",
    "        # Load indexes\n",
    "        self.load_indexes()\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        \"\"\"Load FAISS and BM25 indexes\"\"\"\n",
    "        \n",
    "        # Load FAISS index\n",
    "        faiss_path = os.path.join(self.config['storage']['faiss_index'], 'faiss.index')\n",
    "        if not os.path.exists(faiss_path):\n",
    "            raise FileNotFoundError(f\"FAISS index not found at {faiss_path}\")\n",
    "        self.faiss_index = faiss.read_index(faiss_path)\n",
    "        print(f\"âœ“ FAISS index loaded\")\n",
    "        \n",
    "        # Load embeddings\n",
    "        embeddings_path = os.path.join(self.config['storage']['faiss_index'], 'embeddings.npy')\n",
    "        if os.path.exists(embeddings_path):\n",
    "            self.embeddings = np.load(embeddings_path)\n",
    "            print(f\"âœ“ Embeddings loaded: shape {self.embeddings.shape}\")\n",
    "        \n",
    "        # Load BM25 index\n",
    "        bm25_path = os.path.join(self.config['storage']['bm25_index'], 'bm25.pkl')\n",
    "        if not os.path.exists(bm25_path):\n",
    "            raise FileNotFoundError(f\"BM25 index not found at {bm25_path}\")\n",
    "        with open(bm25_path, 'rb') as f:\n",
    "            self.bm25_index = pickle.load(f)\n",
    "        print(f\"âœ“ BM25 index loaded\")\n",
    "        \n",
    "        # Load chunk metadata (contains full content)\n",
    "        metadata_path = os.path.join(self.config['storage']['faiss_index'], 'chunk_metadata.json')\n",
    "        if not os.path.exists(metadata_path):\n",
    "            raise FileNotFoundError(f\"Chunk metadata not found at {metadata_path}\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        print(f\"âœ“ Chunk metadata loaded: {len(self.chunks)} chunks\")\n",
    "        \n",
    "        print(\"\\nâœ“ All indexes loaded successfully\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get embedding from Qwen SageMaker endpoint (UNCHANGED)\"\"\"\n",
    "        params = {\n",
    "            \"inputs\": [text],\n",
    "            \"encoding_format\": \"float\"\n",
    "        }\n",
    "        body = json.dumps(params)\n",
    "        \n",
    "        response = self.embedding_client.invoke_endpoint(\n",
    "            EndpointName=self.embedding_endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=body\n",
    "        )\n",
    "        output_data = json.loads(response['Body'].read().decode())\n",
    "        embedding = np.array(output_data[0], dtype='float32')\n",
    "        return embedding\n",
    "    \n",
    "    def hybrid_search(self, query: str, entitlement: str, org_id: str = None,\n",
    "                      tags: List[str] = None, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Stage 1: Hybrid search to retrieve candidate chunks.\n",
    "        Returns chunks with FULL CONTENT for re-ranking.\n",
    "        \"\"\"\n",
    "        if top_k is None:\n",
    "            top_k = self.config['retrieval']['hybrid']['top_k']\n",
    "        \n",
    "        # Get query embedding from Qwen\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Retrieve more candidates for re-ranking\n",
    "        retrieval_multiplier = 10\n",
    "        initial_top_k = min(top_k * retrieval_multiplier, len(self.chunks))\n",
    "        \n",
    "        # Vector search (FAISS)\n",
    "        vector_scores, vector_indices = self.faiss_index.search(query_embedding, initial_top_k)\n",
    "        vector_scores = vector_scores[0]\n",
    "        vector_indices = vector_indices[0]\n",
    "        \n",
    "        # Keyword search (BM25)\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize scores\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        vector_scores_norm = normalize(vector_scores)\n",
    "        bm25_scores_norm = normalize(bm25_scores)\n",
    "        \n",
    "        # Compute hybrid scores\n",
    "        vector_weight = self.config['retrieval']['hybrid']['vector_weight']\n",
    "        bm25_weight = self.config['retrieval']['hybrid']['bm25_weight']\n",
    "        \n",
    "        hybrid_scores = {}\n",
    "        for idx, score in zip(vector_indices, vector_scores_norm):\n",
    "            hybrid_scores[idx] = score * vector_weight\n",
    "        \n",
    "        for idx, score in enumerate(bm25_scores_norm):\n",
    "            if idx in hybrid_scores:\n",
    "                hybrid_scores[idx] += score * bm25_weight\n",
    "            else:\n",
    "                hybrid_scores[idx] = score * bm25_weight\n",
    "        \n",
    "        # Sort by hybrid score\n",
    "        sorted_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Filter by entitlements and collect results WITH FULL CONTENT\n",
    "        accessible_results = []\n",
    "        \n",
    "        for idx, score in sorted_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            \n",
    "            # Entitlement filter\n",
    "            chunk_entitlements = chunk['entitlement']\n",
    "            if isinstance(chunk_entitlements, str):\n",
    "                chunk_entitlements = [chunk_entitlements]\n",
    "            \n",
    "            has_access = 'universal' in chunk_entitlements or entitlement in chunk_entitlements\n",
    "            if not has_access:\n",
    "                continue\n",
    "            \n",
    "            # Org filter\n",
    "            if org_id and chunk['orgId'] != org_id:\n",
    "                continue\n",
    "            \n",
    "            # Tag filter\n",
    "            if tags and not any(t in chunk['metadata']['tags'] for t in tags):\n",
    "                continue\n",
    "            \n",
    "            chunk['hybrid_score'] = float(score)\n",
    "            accessible_results.append(chunk)\n",
    "        \n",
    "        accessible_results.sort(key=lambda x: x['hybrid_score'], reverse=True)\n",
    "        return accessible_results[:top_k]\n",
    "    \n",
    "    def rerank(self, query: str, candidates: List[Dict], top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Stage 2: Re-rank candidates using cross-encoder.\n",
    "        \n",
    "        The re-ranker receives FULL DOCUMENT CONTENT, just like the LLM did:\n",
    "        - Query: \"How do I process a cancellation?\"\n",
    "        - Document: Full chunk content text\n",
    "        \n",
    "        The cross-encoder processes both together and outputs a relevance score.\n",
    "        \"\"\"\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        # ============================================================\n",
    "        # Prepare (query, document_content) pairs for re-ranking\n",
    "        # The re-ranker sees the FULL CONTENT, same as LLM context\n",
    "        # ============================================================\n",
    "        pairs = []\n",
    "        for chunk in candidates:\n",
    "            # Full document content goes to re-ranker\n",
    "            document_content = chunk['content']\n",
    "            pairs.append((query, document_content))\n",
    "        \n",
    "        # Get relevance scores from cross-encoder\n",
    "        rerank_scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Add rerank scores to candidates\n",
    "        for i, chunk in enumerate(candidates):\n",
    "            chunk['rerank_score'] = float(rerank_scores[i])\n",
    "        \n",
    "        # Sort by rerank score (highest first)\n",
    "        reranked = sorted(candidates, key=lambda x: x['rerank_score'], reverse=True)\n",
    "        \n",
    "        return reranked[:top_k]\n",
    "    \n",
    "    def query(self, query: str, entitlement: str, org_id: str = None,\n",
    "              tags: List[str] = None, top_k: int = 5,\n",
    "              candidates_for_rerank: int = 20) -> Dict:\n",
    "        \"\"\"\n",
    "        Full retrieval pipeline:\n",
    "        1. Hybrid search gets candidates (with full content)\n",
    "        2. Re-ranker scores each candidate using full content\n",
    "        3. Return top documents by rerank score\n",
    "        \n",
    "        Args:\n",
    "            query: User's search query\n",
    "            entitlement: User's access level\n",
    "            org_id: Organization filter\n",
    "            tags: Tag filters\n",
    "            top_k: Number of final results to return\n",
    "            candidates_for_rerank: Number of candidates to consider for re-ranking\n",
    "        \"\"\"\n",
    "        # Stage 1: Get candidates via hybrid search\n",
    "        candidates = self.hybrid_search(\n",
    "            query=query,\n",
    "            entitlement=entitlement,\n",
    "            org_id=org_id,\n",
    "            tags=tags,\n",
    "            top_k=candidates_for_rerank\n",
    "        )\n",
    "        \n",
    "        if not candidates:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'documents': [],\n",
    "                'message': 'No relevant documents found.'\n",
    "            }\n",
    "        \n",
    "        # Stage 2: Re-rank using full document content\n",
    "        reranked = self.rerank(query, candidates, top_k=top_k)\n",
    "        \n",
    "        # Build response with unique documents\n",
    "        seen_docs = set()\n",
    "        documents = []\n",
    "        \n",
    "        for chunk in reranked:\n",
    "            doc_id = chunk['doc_id']\n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                documents.append({\n",
    "                    'document_name': chunk['title'],\n",
    "                    'doc_id': doc_id,\n",
    "                    'rerank_score': chunk['rerank_score'],\n",
    "                    'hybrid_score': chunk['hybrid_score'],\n",
    "                    'content_preview': chunk['content'][:200] + '...' if len(chunk['content']) > 200 else chunk['content']\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'documents': documents,\n",
    "            'candidates_considered': len(candidates)\n",
    "        }\n",
    "    \n",
    "    # ==================== Session Management ====================\n",
    "    \n",
    "    def create_session(self, user_id: str, entitlement: str, org_id: str = None) -> str:\n",
    "        \"\"\"Create a new session\"\"\"\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            'session_id': session_id,\n",
    "            'user_id': user_id,\n",
    "            'entitlement': entitlement,\n",
    "            'org_id': org_id,\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'query_history': [],\n",
    "            'last_activity': datetime.now().isoformat()\n",
    "        }\n",
    "        print(f\"âœ“ Created session: {session_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def get_session(self, session_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Get session data\"\"\"\n",
    "        return self.sessions.get(session_id)\n",
    "    \n",
    "    def query_with_session(self, session_id: str, query: str,\n",
    "                           tags: List[str] = None, top_k: int = 5,\n",
    "                           candidates_for_rerank: int = 20) -> Dict:\n",
    "        \"\"\"Query with session tracking\"\"\"\n",
    "        session = self.get_session(session_id)\n",
    "        if not session:\n",
    "            raise ValueError(f\"Session {session_id} not found\")\n",
    "        \n",
    "        session['last_activity'] = datetime.now().isoformat()\n",
    "        \n",
    "        result = self.query(\n",
    "            query=query,\n",
    "            entitlement=session['entitlement'],\n",
    "            org_id=session['org_id'],\n",
    "            tags=tags,\n",
    "            top_k=top_k,\n",
    "            candidates_for_rerank=candidates_for_rerank\n",
    "        )\n",
    "        \n",
    "        result['session_id'] = session_id\n",
    "        \n",
    "        # Store in history\n",
    "        session['query_history'].append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'query': query,\n",
    "            'documents_found': [d['document_name'] for d in result['documents']]\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_query_history(self, session_id: str, limit: int = None) -> List[Dict]:\n",
    "        \"\"\"Get query history\"\"\"\n",
    "        session = self.get_session(session_id)\n",
    "        if not session:\n",
    "            return []\n",
    "        history = session['query_history']\n",
    "        return history[-limit:] if limit else history\n",
    "    \n",
    "    def clear_session(self, session_id: str):\n",
    "        \"\"\"Clear session\"\"\"\n",
    "        if session_id in self.sessions:\n",
    "            del self.sessions[session_id]\n",
    "            print(f\"âœ“ Session {session_id} cleared\")\n",
    "    \n",
    "    def export_session(self, session_id: str, filepath: str):\n",
    "        \"\"\"Export session to JSON\"\"\"\n",
    "        session = self.get_session(session_id)\n",
    "        if session:\n",
    "            with open(filepath, 'w') as f:\n",
    "                json.dump(session, f, indent=2)\n",
    "            print(f\"âœ“ Session exported to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ“ ReRankingRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the retriever with re-ranking\n",
    "retriever = ReRankingRetriever(\n",
    "    config=config,\n",
    "    reranker_model=RERANKER_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test: Basic Query with Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEST 1: Basic Query with Re-Ranking\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "result = retriever.query(\n",
    "    query='How do I process a cancellation?',\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    tags=['cancellation'],\n",
    "    top_k=5,\n",
    "    candidates_for_rerank=20\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {result['query']}\")\n",
    "print(f\"Candidates considered: {result.get('candidates_considered', 'N/A')}\")\n",
    "print(f\"\\nRanked Documents:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for i, doc in enumerate(result['documents'], 1):\n",
    "    print(f\"\\n{i}. {doc['document_name']}\")\n",
    "    print(f\"   Re-rank Score: {doc['rerank_score']:.4f}\")\n",
    "    print(f\"   Hybrid Score:  {doc['hybrid_score']:.4f}\")\n",
    "    print(f\"   Preview: {doc['content_preview'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test: Compare Before and After Re-Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 2: Compare Hybrid Search vs Re-Ranked Results\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"What documents are needed for a refund?\"\n",
    "\n",
    "# Get hybrid search results (Stage 1 only)\n",
    "hybrid_only = retriever.hybrid_search(\n",
    "    query=test_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Get full re-ranked results (Stage 1 + Stage 2)\n",
    "reranked = retriever.query(\n",
    "    query=test_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    top_k=5,\n",
    "    candidates_for_rerank=20\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "\n",
    "print(f\"\\n{'BEFORE RE-RANKING (Hybrid Only)':^50}\")\n",
    "print(\"-\"*50)\n",
    "for i, chunk in enumerate(hybrid_only, 1):\n",
    "    print(f\"  {i}. {chunk['title']} (hybrid: {chunk['hybrid_score']:.4f})\")\n",
    "\n",
    "print(f\"\\n{'AFTER RE-RANKING':^50}\")\n",
    "print(\"-\"*50)\n",
    "for i, doc in enumerate(reranked['documents'], 1):\n",
    "    print(f\"  {i}. {doc['document_name']} (rerank: {doc['rerank_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test: Session-Based Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 3: Session-Based Queries\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create session\n",
    "session_id = retriever.create_session(\n",
    "    user_id='agent_001',\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "# Query 1\n",
    "print(\"\\n--- Query 1 ---\")\n",
    "r1 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"How do I cancel a booking?\"\n",
    ")\n",
    "print(f\"Query: {r1['query']}\")\n",
    "print(f\"Top Document: {r1['documents'][0]['document_name'] if r1['documents'] else 'None'}\")\n",
    "\n",
    "# Query 2\n",
    "print(\"\\n--- Query 2 ---\")\n",
    "r2 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"What is the refund timeline?\"\n",
    ")\n",
    "print(f\"Query: {r2['query']}\")\n",
    "print(f\"Top Document: {r2['documents'][0]['document_name'] if r2['documents'] else 'None'}\")\n",
    "\n",
    "# Query 3\n",
    "print(\"\\n--- Query 3 ---\")\n",
    "r3 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"How do I verify customer identity?\"\n",
    ")\n",
    "print(f\"Query: {r3['query']}\")\n",
    "print(f\"Top Document: {r3['documents'][0]['document_name'] if r3['documents'] else 'None'}\")\n",
    "\n",
    "# Show history\n",
    "print(\"\\n--- Session History ---\")\n",
    "history = retriever.get_query_history(session_id)\n",
    "for i, h in enumerate(history, 1):\n",
    "    print(f\"  {i}. [{h['timestamp'][:19]}] {h['query']}\")\n",
    "    print(f\"     â†’ {h['documents_found']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test: Entitlement Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TEST 4: Entitlement-Based Access Control\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "query = \"What are the booking procedures?\"\n",
    "\n",
    "# Support agent query\n",
    "support_result = retriever.query(\n",
    "    query=query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "# Sales agent query\n",
    "sales_result = retriever.query(\n",
    "    query=query,\n",
    "    entitlement='agent_sales',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "\n",
    "print(f\"\\nSupport Agent Results ({len(support_result['documents'])} docs):\")\n",
    "for doc in support_result['documents']:\n",
    "    print(f\"  ðŸ“„ {doc['document_name']} (score: {doc['rerank_score']:.4f})\")\n",
    "\n",
    "print(f\"\\nSales Agent Results ({len(sales_result['documents'])} docs):\")\n",
    "for doc in sales_result['documents']:\n",
    "    print(f\"  ðŸ“„ {doc['document_name']} (score: {doc['rerank_score']:.4f})\")\n",
    "\n",
    "print(\"\\nâœ“ Different entitlements see different documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_PROFILES = {\n",
    "    '1': {'user_id': 'agent_001', 'name': 'Alice (Support)', 'entitlement': 'agent_support', 'org_id': 'org_123'},\n",
    "    '2': {'user_id': 'agent_002', 'name': 'Bob (Sales)', 'entitlement': 'agent_sales', 'org_id': 'org_123'},\n",
    "    '3': {'user_id': 'manager_001', 'name': 'Carol (Manager)', 'entitlement': 'agent_manager', 'org_id': 'org_123'}\n",
    "}\n",
    "\n",
    "def interactive_mode():\n",
    "    \"\"\"Interactive query mode\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Interactive Document Retrieval (with Re-Ranking)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nSelect User Profile:\")\n",
    "    for key, profile in USER_PROFILES.items():\n",
    "        print(f\"  {key}. {profile['name']}\")\n",
    "    \n",
    "    choice = input(\"\\nChoice (1-3): \").strip()\n",
    "    if choice not in USER_PROFILES:\n",
    "        print(\"Invalid choice\")\n",
    "        return\n",
    "    \n",
    "    profile = USER_PROFILES[choice]\n",
    "    session_id = retriever.create_session(\n",
    "        user_id=profile['user_id'],\n",
    "        entitlement=profile['entitlement'],\n",
    "        org_id=profile['org_id']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nLogged in as: {profile['name']}\")\n",
    "    print(f\"Re-ranker: {RERANKER_MODEL}\")\n",
    "    print(\"Commands: 'quit' to exit, 'history' to see past queries\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"You: \").strip()\n",
    "        \n",
    "        if query.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if query.lower() == 'history':\n",
    "            history = retriever.get_query_history(session_id)\n",
    "            print(\"\\nQuery History:\")\n",
    "            for h in history:\n",
    "                print(f\"  [{h['timestamp'][:19]}] {h['query']}\")\n",
    "                print(f\"    â†’ {h['documents_found']}\")\n",
    "            print()\n",
    "            continue\n",
    "        \n",
    "        if not query:\n",
    "            continue\n",
    "        \n",
    "        result = retriever.query_with_session(session_id=session_id, query=query)\n",
    "        \n",
    "        print(\"\\nRelevant Documents:\")\n",
    "        if result['documents']:\n",
    "            for i, doc in enumerate(result['documents'], 1):\n",
    "                print(f\"  {i}. {doc['document_name']} (score: {doc['rerank_score']:.4f})\")\n",
    "        else:\n",
    "            print(\"  No relevant documents found.\")\n",
    "        print()\n",
    "\n",
    "# Uncomment to run interactive mode:\n",
    "# interactive_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### What Changed (LLM â†’ Re-Ranker):\n",
    "\n",
    "| Aspect | Original (Llama LLM) | New (Cross-Encoder Re-Ranker) |\n",
    "|--------|---------------------|-------------------------------|\n",
    "| Input | Query + Full document content | Query + Full document content |\n",
    "| Output | Generated text answer | Relevance score (float) |\n",
    "| Purpose | Answer generation | Document ranking |\n",
    "| Returns | `{'answer': '...'}` | `{'documents': [...]}` |\n",
    "\n",
    "### Pipeline:\n",
    "```\n",
    "Query\n",
    "  â†“\n",
    "Hybrid Search (Qwen embeddings + BM25)\n",
    "  â†“\n",
    "20 Candidate Chunks (with full content)\n",
    "  â†“\n",
    "Cross-Encoder Re-Ranker (scores query vs full content)\n",
    "  â†“\n",
    "Top 5 Ranked Documents\n",
    "```\n",
    "\n",
    "### Key Point:\n",
    "The re-ranker receives the **full document content** (same as the LLM did), not just keywords. It computes a relevance score by processing query and document together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nRe-ranker: {RERANKER_MODEL}\")\n",
    "print(\"\\nThe re-ranker receives FULL document content (like the LLM did).\")\n",
    "print(\"It outputs relevance scores instead of generated answers.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Retrieval with Conversation History (Embedding-Based)\n",
    "\n",
    "This notebook handles conversation history **without a re-ranker** by:\n",
    "\n",
    "1. **Query Expansion** - Append history context to the query before embedding\n",
    "2. **Multi-Query Retrieval** - Search with current query + previous queries, merge results\n",
    "3. **Context-Aware Embedding** - Create a combined embedding from history + current query\n",
    "\n",
    "## Why This Works:\n",
    "```\n",
    "User: \"How do I cancel a booking?\"     → Finds cancellation docs\n",
    "User: \"What about the refund?\"         → Finds refund docs (context: cancellation)\n",
    "User: \"What documents do I need?\"      → Finds cancellation/refund docs (not generic docs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Dict, Optional\n",
    "import numpy as np\n",
    "import faiss\n",
    "import boto3\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "with open('config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Config loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationAwareRetriever:\n",
    "    \"\"\"\n",
    "    Document retriever that uses conversation history to improve results.\n",
    "    Uses embeddings only (no re-ranker).\n",
    "    \n",
    "    Strategies:\n",
    "    1. Query Expansion - Expand query with history context\n",
    "    2. Multi-Query - Search with multiple queries, merge results\n",
    "    3. Embedding Fusion - Combine embeddings from history + current query\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding client\n",
    "        self.embedding_endpoint_name = config['models']['embedding']['endpoint_name']\n",
    "        embedding_creds = config['models']['embedding']['credentials']\n",
    "        self.embedding_client = boto3.client(\n",
    "            'sagemaker-runtime',\n",
    "            region_name=embedding_creds['region'],\n",
    "            aws_access_key_id=embedding_creds['accessKeyId'],\n",
    "            aws_secret_access_key=embedding_creds['secretAccessKey'],\n",
    "            aws_session_token=embedding_creds['sessionToken']\n",
    "        )\n",
    "        print(\"✓ Embedding client initialized\")\n",
    "        \n",
    "        # Session management\n",
    "        self.sessions = {}\n",
    "        \n",
    "        self.load_indexes()\n",
    "    \n",
    "    def load_indexes(self):\n",
    "        faiss_path = os.path.join(self.config['storage']['faiss_index'], 'faiss.index')\n",
    "        self.faiss_index = faiss.read_index(faiss_path)\n",
    "        \n",
    "        bm25_path = os.path.join(self.config['storage']['bm25_index'], 'bm25.pkl')\n",
    "        with open(bm25_path, 'rb') as f:\n",
    "            self.bm25_index = pickle.load(f)\n",
    "        \n",
    "        metadata_path = os.path.join(self.config['storage']['faiss_index'], 'chunk_metadata.json')\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        \n",
    "        print(f\"✓ Indexes loaded: {len(self.chunks)} chunks\")\n",
    "    \n",
    "    def get_embedding(self, text: str) -> np.ndarray:\n",
    "        \"\"\"Get embedding for text\"\"\"\n",
    "        params = {\"inputs\": [text], \"encoding_format\": \"float\"}\n",
    "        response = self.embedding_client.invoke_endpoint(\n",
    "            EndpointName=self.embedding_endpoint_name,\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(params)\n",
    "        )\n",
    "        raw_bytes = response['Body'].read()\n",
    "        output_data = json.loads(raw_bytes.decode())\n",
    "        return np.array(output_data[0], dtype='float32')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STRATEGY 1: Query Expansion\n",
    "    # =========================================================================\n",
    "    def expand_query_with_history(self, query: str, history: List[Dict], \n",
    "                                   max_history: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Expand the query by prepending conversation context.\n",
    "        \n",
    "        Example:\n",
    "            History: [\"How do I cancel?\", \"What's the refund policy?\"]\n",
    "            Query: \"What documents do I need?\"\n",
    "            \n",
    "            Expanded: \"cancel booking refund policy. What documents do I need?\"\n",
    "        \"\"\"\n",
    "        if not history:\n",
    "            return query\n",
    "        \n",
    "        # Get recent history\n",
    "        recent = history[-max_history:]\n",
    "        \n",
    "        # Extract key terms from previous queries\n",
    "        context_terms = []\n",
    "        for turn in recent:\n",
    "            prev_query = turn.get('query', '')\n",
    "            # Add the previous query terms\n",
    "            context_terms.append(prev_query)\n",
    "        \n",
    "        # Combine context with current query\n",
    "        context_str = \" \".join(context_terms)\n",
    "        expanded_query = f\"{context_str}. {query}\"\n",
    "        \n",
    "        return expanded_query\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STRATEGY 2: Multi-Query Retrieval with Fusion\n",
    "    # =========================================================================\n",
    "    def multi_query_search(self, query: str, history: List[Dict],\n",
    "                           entitlement: str, org_id: str = None,\n",
    "                           tags: List[str] = None, top_k: int = 20,\n",
    "                           history_weight: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search with multiple queries (current + history) and fuse results.\n",
    "        \n",
    "        - Current query gets weight: (1 - history_weight)\n",
    "        - History queries share weight: history_weight\n",
    "        \"\"\"\n",
    "        all_scores = {}  # chunk_idx -> score\n",
    "        \n",
    "        # Search with current query (primary)\n",
    "        current_results = self._single_query_search(query, entitlement, org_id, tags, top_k * 2)\n",
    "        current_weight = 1.0 - history_weight\n",
    "        \n",
    "        for chunk in current_results:\n",
    "            idx = self.chunks.index(chunk) if chunk in self.chunks else None\n",
    "            if idx is not None:\n",
    "                chunk_id = chunk.get('doc_id', '') + '_' + str(chunk.get('chunk_index', 0))\n",
    "                all_scores[chunk_id] = {\n",
    "                    'chunk': chunk,\n",
    "                    'score': chunk['hybrid_score'] * current_weight\n",
    "                }\n",
    "        \n",
    "        # Search with history queries\n",
    "        if history:\n",
    "            recent = history[-3:]  # Last 3 turns\n",
    "            per_history_weight = history_weight / len(recent)\n",
    "            \n",
    "            for turn in recent:\n",
    "                prev_query = turn.get('query', '')\n",
    "                if prev_query:\n",
    "                    hist_results = self._single_query_search(prev_query, entitlement, org_id, tags, top_k)\n",
    "                    \n",
    "                    for chunk in hist_results:\n",
    "                        chunk_id = chunk.get('doc_id', '') + '_' + str(chunk.get('chunk_index', 0))\n",
    "                        if chunk_id in all_scores:\n",
    "                            all_scores[chunk_id]['score'] += chunk['hybrid_score'] * per_history_weight\n",
    "                        else:\n",
    "                            all_scores[chunk_id] = {\n",
    "                                'chunk': chunk,\n",
    "                                'score': chunk['hybrid_score'] * per_history_weight\n",
    "                            }\n",
    "        \n",
    "        # Sort by fused score\n",
    "        sorted_results = sorted(all_scores.values(), key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Update scores and return\n",
    "        results = []\n",
    "        for item in sorted_results[:top_k]:\n",
    "            chunk = item['chunk'].copy()\n",
    "            chunk['fused_score'] = item['score']\n",
    "            results.append(chunk)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # =========================================================================\n",
    "    # STRATEGY 3: Embedding Fusion\n",
    "    # =========================================================================\n",
    "    def get_fused_embedding(self, query: str, history: List[Dict],\n",
    "                            current_weight: float = 0.7,\n",
    "                            history_weight: float = 0.3) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Create a fused embedding from current query + history.\n",
    "        \n",
    "        fused = current_weight * embed(query) + history_weight * mean(embed(history))\n",
    "        \"\"\"\n",
    "        # Get current query embedding\n",
    "        current_embedding = self.get_embedding(query)\n",
    "        \n",
    "        if not history:\n",
    "            return current_embedding\n",
    "        \n",
    "        # Get history embeddings\n",
    "        recent = history[-3:]\n",
    "        history_embeddings = []\n",
    "        \n",
    "        for turn in recent:\n",
    "            prev_query = turn.get('query', '')\n",
    "            if prev_query:\n",
    "                emb = self.get_embedding(prev_query)\n",
    "                history_embeddings.append(emb)\n",
    "        \n",
    "        if not history_embeddings:\n",
    "            return current_embedding\n",
    "        \n",
    "        # Average history embeddings\n",
    "        history_mean = np.mean(history_embeddings, axis=0)\n",
    "        \n",
    "        # Fuse embeddings\n",
    "        fused = current_weight * current_embedding + history_weight * history_mean\n",
    "        \n",
    "        # Normalize\n",
    "        fused = fused / np.linalg.norm(fused)\n",
    "        \n",
    "        return fused.astype('float32')\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Core Search Methods\n",
    "    # =========================================================================\n",
    "    def _single_query_search(self, query: str, entitlement: str, \n",
    "                              org_id: str = None, tags: List[str] = None,\n",
    "                              top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"Basic hybrid search for a single query\"\"\"\n",
    "        query_embedding = self.get_embedding(query)\n",
    "        return self._search_with_embedding(query, query_embedding, entitlement, org_id, tags, top_k)\n",
    "    \n",
    "    def _search_with_embedding(self, query: str, query_embedding: np.ndarray,\n",
    "                                entitlement: str, org_id: str = None,\n",
    "                                tags: List[str] = None, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"Search using a pre-computed embedding\"\"\"\n",
    "        query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        initial_top_k = min(top_k * 10, len(self.chunks))\n",
    "        \n",
    "        # Vector search\n",
    "        vector_scores, vector_indices = self.faiss_index.search(query_embedding, initial_top_k)\n",
    "        vector_scores = vector_scores[0]\n",
    "        vector_indices = vector_indices[0]\n",
    "        \n",
    "        # BM25 search\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = self.bm25_index.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalize\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-10:\n",
    "                return np.zeros_like(scores)\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        \n",
    "        vector_scores_norm = normalize(vector_scores)\n",
    "        bm25_scores_norm = normalize(bm25_scores)\n",
    "        \n",
    "        # Hybrid scores\n",
    "        vector_weight = self.config['retrieval']['hybrid']['vector_weight']\n",
    "        bm25_weight = self.config['retrieval']['hybrid']['bm25_weight']\n",
    "        \n",
    "        hybrid_scores = {}\n",
    "        for idx, score in zip(vector_indices, vector_scores_norm):\n",
    "            hybrid_scores[idx] = score * vector_weight\n",
    "        \n",
    "        for idx, score in enumerate(bm25_scores_norm):\n",
    "            if idx in hybrid_scores:\n",
    "                hybrid_scores[idx] += score * bm25_weight\n",
    "            else:\n",
    "                hybrid_scores[idx] = score * bm25_weight\n",
    "        \n",
    "        sorted_indices = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Filter by access\n",
    "        results = []\n",
    "        for idx, score in sorted_indices:\n",
    "            chunk = self.chunks[idx].copy()\n",
    "            \n",
    "            chunk_entitlements = chunk['entitlement']\n",
    "            if isinstance(chunk_entitlements, str):\n",
    "                chunk_entitlements = [chunk_entitlements]\n",
    "            \n",
    "            has_access = 'universal' in chunk_entitlements or entitlement in chunk_entitlements\n",
    "            if not has_access:\n",
    "                continue\n",
    "            if org_id and chunk['orgId'] != org_id:\n",
    "                continue\n",
    "            if tags and not any(t in chunk['metadata']['tags'] for t in tags):\n",
    "                continue\n",
    "            \n",
    "            chunk['hybrid_score'] = float(score)\n",
    "            results.append(chunk)\n",
    "        \n",
    "        return results[:top_k]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Main Query Methods\n",
    "    # =========================================================================\n",
    "    def query(self, query: str, entitlement: str, org_id: str = None,\n",
    "              tags: List[str] = None, top_k: int = 5,\n",
    "              conversation_history: List[Dict] = None,\n",
    "              strategy: str = 'query_expansion') -> Dict:\n",
    "        \"\"\"\n",
    "        Query with conversation history support.\n",
    "        \n",
    "        Args:\n",
    "            strategy: 'query_expansion', 'multi_query', or 'embedding_fusion'\n",
    "        \"\"\"\n",
    "        history = conversation_history or []\n",
    "        \n",
    "        if strategy == 'query_expansion':\n",
    "            # Strategy 1: Expand query with history\n",
    "            expanded_query = self.expand_query_with_history(query, history)\n",
    "            results = self._single_query_search(expanded_query, entitlement, org_id, tags, top_k * 2)\n",
    "            # Also search with original query and merge\n",
    "            original_results = self._single_query_search(query, entitlement, org_id, tags, top_k)\n",
    "            results = self._merge_results(results, original_results, top_k)\n",
    "            \n",
    "        elif strategy == 'multi_query':\n",
    "            # Strategy 2: Multi-query fusion\n",
    "            results = self.multi_query_search(query, history, entitlement, org_id, tags, top_k * 2)\n",
    "            \n",
    "        elif strategy == 'embedding_fusion':\n",
    "            # Strategy 3: Fused embedding\n",
    "            fused_embedding = self.get_fused_embedding(query, history)\n",
    "            results = self._search_with_embedding(query, fused_embedding, entitlement, org_id, tags, top_k * 2)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "        \n",
    "        # Build response\n",
    "        documents = []\n",
    "        seen_docs = set()\n",
    "        for chunk in results[:top_k]:\n",
    "            doc_id = chunk['doc_id']\n",
    "            if doc_id not in seen_docs:\n",
    "                seen_docs.add(doc_id)\n",
    "                documents.append({\n",
    "                    'document_name': chunk['title'],\n",
    "                    'doc_id': doc_id,\n",
    "                    'score': chunk.get('fused_score', chunk.get('hybrid_score', 0))\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'documents': documents,\n",
    "            'strategy': strategy,\n",
    "            'history_turns_used': len(history)\n",
    "        }\n",
    "    \n",
    "    def _merge_results(self, results1: List[Dict], results2: List[Dict], top_k: int) -> List[Dict]:\n",
    "        \"\"\"Merge two result lists, combining scores for duplicates\"\"\"\n",
    "        merged = {}\n",
    "        \n",
    "        for chunk in results1:\n",
    "            chunk_id = chunk.get('doc_id', '') + '_' + str(chunk.get('chunk_index', 0))\n",
    "            merged[chunk_id] = chunk.copy()\n",
    "            merged[chunk_id]['fused_score'] = chunk.get('hybrid_score', 0)\n",
    "        \n",
    "        for chunk in results2:\n",
    "            chunk_id = chunk.get('doc_id', '') + '_' + str(chunk.get('chunk_index', 0))\n",
    "            if chunk_id in merged:\n",
    "                # Boost score if found in both\n",
    "                merged[chunk_id]['fused_score'] += chunk.get('hybrid_score', 0) * 0.5\n",
    "            else:\n",
    "                merged[chunk_id] = chunk.copy()\n",
    "                merged[chunk_id]['fused_score'] = chunk.get('hybrid_score', 0) * 0.5\n",
    "        \n",
    "        sorted_results = sorted(merged.values(), key=lambda x: x['fused_score'], reverse=True)\n",
    "        return sorted_results[:top_k]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Session Management\n",
    "    # =========================================================================\n",
    "    def create_session(self, user_id: str, entitlement: str, org_id: str = None) -> str:\n",
    "        session_id = str(uuid.uuid4())\n",
    "        self.sessions[session_id] = {\n",
    "            'session_id': session_id,\n",
    "            'user_id': user_id,\n",
    "            'entitlement': entitlement,\n",
    "            'org_id': org_id,\n",
    "            'query_history': []\n",
    "        }\n",
    "        print(f\"✓ Created session: {session_id}\")\n",
    "        return session_id\n",
    "    \n",
    "    def query_with_session(self, session_id: str, query: str,\n",
    "                           tags: List[str] = None, top_k: int = 5,\n",
    "                           strategy: str = 'query_expansion',\n",
    "                           history_limit: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Query using session history.\n",
    "        \"\"\"\n",
    "        session = self.sessions.get(session_id)\n",
    "        if not session:\n",
    "            raise ValueError(f\"Session not found: {session_id}\")\n",
    "        \n",
    "        # Get recent history\n",
    "        history = session['query_history'][-history_limit:] if history_limit else session['query_history']\n",
    "        \n",
    "        # Perform query\n",
    "        result = self.query(\n",
    "            query=query,\n",
    "            entitlement=session['entitlement'],\n",
    "            org_id=session['org_id'],\n",
    "            tags=tags,\n",
    "            top_k=top_k,\n",
    "            conversation_history=history,\n",
    "            strategy=strategy\n",
    "        )\n",
    "        \n",
    "        # Store in history\n",
    "        session['query_history'].append({\n",
    "            'query': query,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'documents_found': [d['document_name'] for d in result['documents']]\n",
    "        })\n",
    "        \n",
    "        result['session_id'] = session_id\n",
    "        return result\n",
    "    \n",
    "    def get_session_history(self, session_id: str) -> List[Dict]:\n",
    "        session = self.sessions.get(session_id)\n",
    "        return session['query_history'] if session else []\n",
    "\n",
    "print(\"✓ ConversationAwareRetriever class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = ConversationAwareRetriever(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Multi-Turn Conversation\n",
    "\n",
    "Watch how the context affects results for ambiguous queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"TEST: Multi-Turn Conversation with History Context\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create session\n",
    "session_id = retriever.create_session(\n",
    "    user_id='agent_001',\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123'\n",
    ")\n",
    "\n",
    "# Turn 1: Establish context (cancellation)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"TURN 1: Establishing context\")\n",
    "print(\"-\"*50)\n",
    "r1 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"How do I cancel a booking?\",\n",
    "    strategy='query_expansion'\n",
    ")\n",
    "print(f\"Query: {r1['query']}\")\n",
    "print(f\"Strategy: {r1['strategy']}\")\n",
    "print(f\"History turns: {r1['history_turns_used']}\")\n",
    "print(f\"Results: {[d['document_name'] for d in r1['documents'][:3]]}\")\n",
    "\n",
    "# Turn 2: Related question\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"TURN 2: Related question\")\n",
    "print(\"-\"*50)\n",
    "r2 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"What about the refund?\",\n",
    "    strategy='query_expansion'\n",
    ")\n",
    "print(f\"Query: {r2['query']}\")\n",
    "print(f\"History turns: {r2['history_turns_used']}\")\n",
    "print(f\"Results: {[d['document_name'] for d in r2['documents'][:3]]}\")\n",
    "\n",
    "# Turn 3: Ambiguous query (should use context)\n",
    "print(\"\\n\" + \"-\"*50)\n",
    "print(\"TURN 3: Ambiguous query (history provides context)\")\n",
    "print(\"-\"*50)\n",
    "r3 = retriever.query_with_session(\n",
    "    session_id=session_id,\n",
    "    query=\"What documents do I need?\",\n",
    "    strategy='query_expansion'\n",
    ")\n",
    "print(f\"Query: {r3['query']}\")\n",
    "print(f\"History turns: {r3['history_turns_used']}\")\n",
    "print(f\"Results: {[d['document_name'] for d in r3['documents'][:3]]}\")\n",
    "print(\"\\n→ With context, 'documents' means cancellation/refund documents, not generic docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: With vs Without History Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: Same Query With vs Without History\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ambiguous_query = \"What documents do I need?\"\n",
    "\n",
    "# Simulated history about cancellations\n",
    "cancellation_history = [\n",
    "    {'query': 'How do I cancel a booking?', 'documents_found': ['Cancellation Policy']},\n",
    "    {'query': 'What is the refund timeline?', 'documents_found': ['Refund Guide']}\n",
    "]\n",
    "\n",
    "# WITHOUT history\n",
    "print(\"\\n--- WITHOUT HISTORY ---\")\n",
    "result_no_history = retriever.query(\n",
    "    query=ambiguous_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    conversation_history=None,\n",
    "    strategy='query_expansion'\n",
    ")\n",
    "print(f\"Query used: '{ambiguous_query}'\")\n",
    "for i, doc in enumerate(result_no_history['documents'][:3], 1):\n",
    "    print(f\"  {i}. {doc['document_name']}\")\n",
    "\n",
    "# WITH history\n",
    "print(\"\\n--- WITH CANCELLATION HISTORY ---\")\n",
    "result_with_history = retriever.query(\n",
    "    query=ambiguous_query,\n",
    "    entitlement='agent_support',\n",
    "    org_id='org_123',\n",
    "    conversation_history=cancellation_history,\n",
    "    strategy='query_expansion'\n",
    ")\n",
    "expanded = retriever.expand_query_with_history(ambiguous_query, cancellation_history)\n",
    "print(f\"Expanded query: '{expanded}'\")\n",
    "for i, doc in enumerate(result_with_history['documents'][:3], 1):\n",
    "    print(f\"  {i}. {doc['document_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Three Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARE: All Three History Strategies\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"What documents do I need?\"\n",
    "test_history = [\n",
    "    {'query': 'How do I cancel a booking?'},\n",
    "    {'query': 'What is the refund policy?'}\n",
    "]\n",
    "\n",
    "for strategy in ['query_expansion', 'multi_query', 'embedding_fusion']:\n",
    "    print(f\"\\n--- Strategy: {strategy} ---\")\n",
    "    result = retriever.query(\n",
    "        query=test_query,\n",
    "        entitlement='agent_support',\n",
    "        org_id='org_123',\n",
    "        conversation_history=test_history,\n",
    "        strategy=strategy,\n",
    "        top_k=3\n",
    "    )\n",
    "    for i, doc in enumerate(result['documents'], 1):\n",
    "        print(f\"  {i}. {doc['document_name']} (score: {doc['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Three Strategies\n",
    "\n",
    "| Strategy | How It Works | Best For |\n",
    "|----------|--------------|----------|\n",
    "| `query_expansion` | Prepends history queries to current query | Simple, fast |\n",
    "| `multi_query` | Searches each query separately, merges results | Diverse results |\n",
    "| `embedding_fusion` | Averages embeddings from history + current | Smooth context blending |\n",
    "\n",
    "### Recommended: `query_expansion`\n",
    "- Simplest approach\n",
    "- Only 1 embedding API call\n",
    "- Works well for follow-up questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou now have conversation history support WITHOUT a re-ranker!\")\n",
    "print(\"\\nRecommended strategy: 'query_expansion'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
